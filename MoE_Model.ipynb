{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPqABpNxehFzyJXudKbUMJG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/weagan/MoE/blob/main/MoE_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Mixture of Experts (MoE) Implementation\n",
        "Supports 0, 1, or 2 GPUs with full technical requirements\n",
        "\"\"\"\n",
        "\n",
        "# ============================================================================\n",
        "# INSTALLATION & IMPORTS\n",
        "# ============================================================================\n",
        "\n",
        "# Install required packages (uncomment if needed)\n",
        "# !pip install torch transformers datasets accelerate\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import math\n",
        "from typing import Tuple, Optional\n",
        "import numpy as np\n",
        "\n",
        "# Check GPU availability\n",
        "device_count = torch.cuda.device_count()\n",
        "print(f\"Available GPUs: {device_count}\")\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# ============================================================================\n",
        "# EXPERT NETWORK\n",
        "# ============================================================================\n",
        "\n",
        "class Expert(nn.Module):\n",
        "    \"\"\"Single expert feedforward network\"\"\"\n",
        "    def __init__(self, input_dim: int, hidden_dim: int, dropout: float = 0.1):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim, input_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# ROUTER WITH GATING MECHANISM\n",
        "# ============================================================================\n",
        "\n",
        "class Router(nn.Module):\n",
        "    \"\"\"Gating network that routes tokens to experts\"\"\"\n",
        "    def __init__(self, input_dim: int, num_experts: int):\n",
        "        super().__init__()\n",
        "        self.gate = nn.Linear(input_dim, num_experts, bias=False)\n",
        "        self.num_experts = num_experts\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: [batch_size, seq_len, input_dim]\n",
        "        logits = self.gate(x)  # [batch_size, seq_len, num_experts]\n",
        "        return logits\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# MIXTURE OF EXPERTS LAYER\n",
        "# ============================================================================\n",
        "\n",
        "class MixtureOfExpertsLayer(nn.Module):\n",
        "    \"\"\"\n",
        "    Complete MoE layer with all technical requirements:\n",
        "    - Multiple expert networks\n",
        "    - Gating/routing mechanism\n",
        "    - Sparse activation (top-k)\n",
        "    - Shared experts\n",
        "    - Load balancing loss\n",
        "    - Router Z-loss\n",
        "    - Capacity constraints\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_dim: int,\n",
        "        hidden_dim: int,\n",
        "        num_experts: int = 8,\n",
        "        num_shared_experts: int = 2,\n",
        "        top_k: int = 2,\n",
        "        capacity_factor: float = 1.25,\n",
        "        load_balance_weight: float = 0.01,\n",
        "        z_loss_weight: float = 0.001,\n",
        "        dropout: float = 0.1\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.input_dim = input_dim\n",
        "        self.num_experts = num_experts\n",
        "        self.num_shared_experts = num_shared_experts\n",
        "        self.top_k = top_k\n",
        "        self.capacity_factor = capacity_factor\n",
        "        self.load_balance_weight = load_balance_weight\n",
        "        self.z_loss_weight = z_loss_weight\n",
        "\n",
        "        # Sparse experts (specialized)\n",
        "        self.experts = nn.ModuleList([\n",
        "            Expert(input_dim, hidden_dim, dropout)\n",
        "            for _ in range(num_experts)\n",
        "        ])\n",
        "\n",
        "        # Shared experts (always active)\n",
        "        self.shared_experts = nn.ModuleList([\n",
        "            Expert(input_dim, hidden_dim, dropout)\n",
        "            for _ in range(num_shared_experts)\n",
        "        ])\n",
        "\n",
        "        # Router\n",
        "        self.router = Router(input_dim, num_experts)\n",
        "\n",
        "        # For tracking auxiliary losses\n",
        "        self.aux_loss = 0.0\n",
        "\n",
        "    def compute_load_balance_loss(\n",
        "        self,\n",
        "        router_probs: torch.Tensor,\n",
        "        expert_mask: torch.Tensor\n",
        "    ) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Load Balancing Loss: L_bal = N * sum(f_i * P_i)\n",
        "\n",
        "        Args:\n",
        "            router_probs: [batch_size, seq_len, num_experts] - router probabilities\n",
        "            expert_mask: [batch_size, seq_len, num_experts] - binary mask of selected experts\n",
        "        \"\"\"\n",
        "        # P_i: mean probability allocated to expert i\n",
        "        P = router_probs.mean(dim=[0, 1])  # [num_experts]\n",
        "\n",
        "        # f_i: fraction of tokens routed to expert i\n",
        "        f = expert_mask.float().mean(dim=[0, 1])  # [num_experts]\n",
        "\n",
        "        # L_bal = N * sum(f_i * P_i)\n",
        "        load_balance_loss = self.num_experts * torch.sum(f * P)\n",
        "\n",
        "        return load_balance_loss\n",
        "\n",
        "    def compute_router_z_loss(self, router_logits: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Router Z-Loss: L_z = (1/B) * sum(log(sum(e^x_i)))^2\n",
        "        Encourages router logits to stay close to zero for stability\n",
        "\n",
        "        Args:\n",
        "            router_logits: [batch_size, seq_len, num_experts]\n",
        "        \"\"\"\n",
        "        # Log-sum-exp for numerical stability\n",
        "        log_sum_exp = torch.logsumexp(router_logits, dim=-1)  # [batch_size, seq_len]\n",
        "\n",
        "        # L_z = mean of squared log-sum-exp\n",
        "        z_loss = torch.mean(log_sum_exp ** 2)\n",
        "\n",
        "        return z_loss\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: [batch_size, seq_len, input_dim]\n",
        "        Returns:\n",
        "            output: [batch_size, seq_len, input_dim]\n",
        "        \"\"\"\n",
        "        batch_size, seq_len, input_dim = x.shape\n",
        "\n",
        "        # Flatten for routing\n",
        "        x_flat = x.view(-1, input_dim)  # [batch_size * seq_len, input_dim]\n",
        "\n",
        "        # ====================================================================\n",
        "        # ROUTING: Compute router logits and probabilities\n",
        "        # ====================================================================\n",
        "        router_logits = self.router(x)  # [batch_size, seq_len, num_experts]\n",
        "        router_logits_flat = router_logits.view(-1, self.num_experts)\n",
        "        router_probs = F.softmax(router_logits_flat, dim=-1)  # [batch_size * seq_len, num_experts]\n",
        "\n",
        "        # ====================================================================\n",
        "        # SPARSE ACTIVATION: Top-k expert selection\n",
        "        # ====================================================================\n",
        "        top_k_probs, top_k_indices = torch.topk(\n",
        "            router_probs,\n",
        "            self.top_k,\n",
        "            dim=-1\n",
        "        )  # Both: [batch_size * seq_len, top_k]\n",
        "\n",
        "        # Normalize top-k probabilities\n",
        "        top_k_probs = top_k_probs / top_k_probs.sum(dim=-1, keepdim=True)\n",
        "\n",
        "        # Create expert mask for load balancing\n",
        "        expert_mask = torch.zeros_like(router_probs)  # [batch_size * seq_len, num_experts]\n",
        "        expert_mask.scatter_(1, top_k_indices, 1.0)\n",
        "        expert_mask = expert_mask.view(batch_size, seq_len, self.num_experts)\n",
        "\n",
        "        # ====================================================================\n",
        "        # CAPACITY CONSTRAINTS: Limit tokens per expert\n",
        "        # ====================================================================\n",
        "        capacity = int(self.capacity_factor * (batch_size * seq_len) / self.num_experts)\n",
        "\n",
        "        # ====================================================================\n",
        "        # EXPERT COMPUTATION: Process tokens through selected experts\n",
        "        # ====================================================================\n",
        "        output = torch.zeros_like(x_flat)  # [batch_size * seq_len, input_dim]\n",
        "\n",
        "        # Process each token\n",
        "        for i in range(batch_size * seq_len):\n",
        "            token = x_flat[i:i+1]  # [1, input_dim]\n",
        "            token_output = torch.zeros_like(token)\n",
        "\n",
        "            # Process through top-k experts\n",
        "            for j, expert_idx in enumerate(top_k_indices[i]):\n",
        "                expert = self.experts[expert_idx]\n",
        "                expert_out = expert(token)\n",
        "                token_output += top_k_probs[i, j] * expert_out\n",
        "\n",
        "            output[i] = token_output\n",
        "\n",
        "        # ====================================================================\n",
        "        # SHARED EXPERTS: Always active for general knowledge\n",
        "        # ====================================================================\n",
        "        shared_output = torch.zeros_like(x_flat)\n",
        "        for shared_expert in self.shared_experts:\n",
        "            shared_output += shared_expert(x_flat)\n",
        "        shared_output = shared_output / self.num_shared_experts\n",
        "\n",
        "        # Combine sparse and shared expert outputs\n",
        "        output = output + shared_output\n",
        "\n",
        "        # Reshape back\n",
        "        output = output.view(batch_size, seq_len, input_dim)\n",
        "\n",
        "        # ====================================================================\n",
        "        # AUXILIARY LOSSES\n",
        "        # ====================================================================\n",
        "        # Load balancing loss\n",
        "        load_balance_loss = self.compute_load_balance_loss(\n",
        "            router_probs.view(batch_size, seq_len, self.num_experts),\n",
        "            expert_mask\n",
        "        )\n",
        "\n",
        "        # Router Z-loss\n",
        "        z_loss = self.compute_router_z_loss(router_logits)\n",
        "\n",
        "        # Store auxiliary loss\n",
        "        self.aux_loss = (\n",
        "            self.load_balance_weight * load_balance_loss +\n",
        "            self.z_loss_weight * z_loss\n",
        "        )\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# COMPLETE MOE MODEL\n",
        "# ============================================================================\n",
        "\n",
        "class MoETransformerBlock(nn.Module):\n",
        "    \"\"\"Transformer block with MoE layer\"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        embed_dim: int,\n",
        "        num_heads: int,\n",
        "        moe_config: dict,\n",
        "        dropout: float = 0.1\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        # Self-attention\n",
        "        self.attention = nn.MultiheadAttention(\n",
        "            embed_dim,\n",
        "            num_heads,\n",
        "            dropout=dropout,\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.norm1 = nn.LayerNorm(embed_dim)\n",
        "\n",
        "        # MoE layer\n",
        "        self.moe = MixtureOfExpertsLayer(\n",
        "            input_dim=embed_dim,\n",
        "            **moe_config\n",
        "        )\n",
        "        self.norm2 = nn.LayerNorm(embed_dim)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x: torch.Tensor, mask: Optional[torch.Tensor] = None):\n",
        "        # Self-attention with residual\n",
        "        attn_out, _ = self.attention(x, x, x, attn_mask=mask)\n",
        "        x = self.norm1(x + self.dropout(attn_out))\n",
        "\n",
        "        # MoE with residual\n",
        "        moe_out = self.moe(x)\n",
        "        x = self.norm2(x + self.dropout(moe_out))\n",
        "\n",
        "        return x, self.moe.aux_loss\n",
        "\n",
        "\n",
        "class MoEModel(nn.Module):\n",
        "    \"\"\"Complete MoE model for sequence tasks\"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        vocab_size: int,\n",
        "        embed_dim: int = 256,\n",
        "        num_layers: int = 4,\n",
        "        num_heads: int = 4,\n",
        "        num_experts: int = 8,\n",
        "        num_shared_experts: int = 2,\n",
        "        top_k: int = 2,\n",
        "        hidden_dim: int = 512,\n",
        "        max_seq_len: int = 512,\n",
        "        dropout: float = 0.1\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.embed_dim = embed_dim\n",
        "\n",
        "        # Embeddings\n",
        "        self.token_embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.position_embedding = nn.Embedding(max_seq_len, embed_dim)\n",
        "\n",
        "        # MoE config\n",
        "        moe_config = {\n",
        "            'hidden_dim': hidden_dim,\n",
        "            'num_experts': num_experts,\n",
        "            'num_shared_experts': num_shared_experts,\n",
        "            'top_k': top_k,\n",
        "            'dropout': dropout\n",
        "        }\n",
        "\n",
        "        # Transformer blocks with MoE\n",
        "        self.layers = nn.ModuleList([\n",
        "            MoETransformerBlock(embed_dim, num_heads, moe_config, dropout)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "        # Output head\n",
        "        self.output_norm = nn.LayerNorm(embed_dim)\n",
        "        self.output_projection = nn.Linear(embed_dim, vocab_size)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # Initialize weights\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, input_ids: torch.Tensor, mask: Optional[torch.Tensor] = None):\n",
        "        batch_size, seq_len = input_ids.shape\n",
        "\n",
        "        # Embeddings\n",
        "        positions = torch.arange(seq_len, device=input_ids.device).unsqueeze(0)\n",
        "        x = self.token_embedding(input_ids) + self.position_embedding(positions)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        # Accumulate auxiliary losses\n",
        "        total_aux_loss = 0.0\n",
        "\n",
        "        # Pass through transformer blocks\n",
        "        for layer in self.layers:\n",
        "            x, aux_loss = layer(x, mask)\n",
        "            total_aux_loss += aux_loss\n",
        "\n",
        "        # Output\n",
        "        x = self.output_norm(x)\n",
        "        logits = self.output_projection(x)\n",
        "\n",
        "        return logits, total_aux_loss\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# TRAINING UTILITIES\n",
        "# ============================================================================\n",
        "\n",
        "class SimpleTextDataset(Dataset):\n",
        "    \"\"\"Simple dataset for demonstration\"\"\"\n",
        "    def __init__(self, num_samples: int = 1000, seq_len: int = 64, vocab_size: int = 1000):\n",
        "        self.num_samples = num_samples\n",
        "        self.seq_len = seq_len\n",
        "        self.vocab_size = vocab_size\n",
        "\n",
        "        # Generate random sequences\n",
        "        self.data = torch.randint(0, vocab_size, (num_samples, seq_len))\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_samples\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx]\n",
        "\n",
        "\n",
        "def train_step(model, batch, optimizer, device):\n",
        "    \"\"\"Single training step\"\"\"\n",
        "    model.train()\n",
        "\n",
        "    # Move to device\n",
        "    input_ids = batch.to(device)\n",
        "\n",
        "    # Shift for next-token prediction\n",
        "    inputs = input_ids[:, :-1]\n",
        "    targets = input_ids[:, 1:]\n",
        "\n",
        "    # Forward pass\n",
        "    logits, aux_loss = model(inputs)\n",
        "\n",
        "    # Compute main loss (cross-entropy)\n",
        "    main_loss = F.cross_entropy(\n",
        "        logits.reshape(-1, logits.size(-1)),\n",
        "        targets.reshape(-1)\n",
        "    )\n",
        "\n",
        "    # Total loss includes auxiliary losses\n",
        "    total_loss = main_loss + aux_loss\n",
        "\n",
        "    # Backward pass\n",
        "    optimizer.zero_grad()\n",
        "    total_loss.backward()\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "    optimizer.step()\n",
        "\n",
        "    return {\n",
        "        'total_loss': total_loss.item(),\n",
        "        'main_loss': main_loss.item(),\n",
        "        'aux_loss': aux_loss.item()\n",
        "    }\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# MULTI-GPU SUPPORT\n",
        "# ============================================================================\n",
        "\n",
        "def setup_distributed_model(model, device_count):\n",
        "    \"\"\"Setup model for multi-GPU training\"\"\"\n",
        "    if device_count > 1:\n",
        "        print(f\"Using DataParallel with {device_count} GPUs\")\n",
        "        model = nn.DataParallel(model)\n",
        "    return model\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# MAIN TRAINING LOOP\n",
        "# ============================================================================\n",
        "\n",
        "def main():\n",
        "    # Hyperparameters\n",
        "    vocab_size = 1000\n",
        "    embed_dim = 256\n",
        "    num_layers = 4\n",
        "    num_heads = 4\n",
        "    num_experts = 8\n",
        "    num_shared_experts = 2\n",
        "    top_k = 2\n",
        "    hidden_dim = 512\n",
        "    batch_size = 32\n",
        "    num_epochs = 3\n",
        "    learning_rate = 3e-4\n",
        "\n",
        "    # Create model\n",
        "    model = MoEModel(\n",
        "        vocab_size=vocab_size,\n",
        "        embed_dim=embed_dim,\n",
        "        num_layers=num_layers,\n",
        "        num_heads=num_heads,\n",
        "        num_experts=num_experts,\n",
        "        num_shared_experts=num_shared_experts,\n",
        "        top_k=top_k,\n",
        "        hidden_dim=hidden_dim\n",
        "    )\n",
        "\n",
        "    # Setup multi-GPU if available\n",
        "    model = setup_distributed_model(model, device_count)\n",
        "    model = model.to(device)\n",
        "\n",
        "    # Print model info\n",
        "    num_params = sum(p.numel() for p in model.parameters())\n",
        "    print(f\"\\nModel Parameters: {num_params:,}\")\n",
        "    print(f\"Number of Experts: {num_experts}\")\n",
        "    print(f\"Number of Shared Experts: {num_shared_experts}\")\n",
        "    print(f\"Top-K Experts: {top_k}\")\n",
        "\n",
        "    # Create dataset and dataloader\n",
        "    dataset = SimpleTextDataset(num_samples=1000, seq_len=64, vocab_size=vocab_size)\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    # Optimizer\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    # Training loop\n",
        "    print(\"\\nStarting training...\")\n",
        "    for epoch in range(num_epochs):\n",
        "        epoch_losses = {'total': 0, 'main': 0, 'aux': 0}\n",
        "\n",
        "        for i, batch in enumerate(dataloader):\n",
        "            losses = train_step(model, batch, optimizer, device)\n",
        "\n",
        "            epoch_losses['total'] += losses['total_loss']\n",
        "            epoch_losses['main'] += losses['main_loss']\n",
        "            epoch_losses['aux'] += losses['aux_loss']\n",
        "\n",
        "            if (i + 1) % 10 == 0:\n",
        "                print(f\"Epoch {epoch+1}/{num_epochs}, Step {i+1}/{len(dataloader)}, \"\n",
        "                      f\"Loss: {losses['total_loss']:.4f} \"\n",
        "                      f\"(Main: {losses['main_loss']:.4f}, Aux: {losses['aux_loss']:.4f})\")\n",
        "\n",
        "        avg_losses = {k: v / len(dataloader) for k, v in epoch_losses.items()}\n",
        "        print(f\"\\nEpoch {epoch+1} Summary - \"\n",
        "              f\"Avg Loss: {avg_losses['total']:.4f} \"\n",
        "              f\"(Main: {avg_losses['main']:.4f}, Aux: {avg_losses['aux']:.4f})\\n\")\n",
        "\n",
        "    print(\"Training complete!\")\n",
        "\n",
        "    return model\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h2t8X7sVrNo-",
        "outputId": "164fbdd5-8c2f-46f9-97b6-797b0a6c4948"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Available GPUs: 1\n",
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "870a6575"
      },
      "source": [
        "# Task\n",
        "Download Shakespeare's complete works from \"https://www.gutenberg.org/files/100/100-0.txt\" and save it as \"shakespeare.txt\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3b1ad13e"
      },
      "source": [
        "## Download Shakespeare Dataset\n",
        "\n",
        "### Subtask:\n",
        "Download Shakespeare's complete works from the provided URL and save it as 'shakespeare.txt'.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7b622d56"
      },
      "source": [
        "**Reasoning**:\n",
        "To download the Shakespeare dataset, I will use the `requests` library to fetch the content from the specified URL and save it to a local file named 'shakespeare.txt', as per the instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a76127c6",
        "outputId": "f65292f7-4ea6-4dc5-96b6-6d82084479d0"
      },
      "source": [
        "import requests\n",
        "\n",
        "# Define the URL for Shakespeare's complete works\n",
        "url = 'https://www.gutenberg.org/files/100/100-0.txt'\n",
        "\n",
        "# Define the local filename to save the content\n",
        "filename = 'shakespeare.txt'\n",
        "\n",
        "print(f\"Attempting to download from: {url}\")\n",
        "\n",
        "try:\n",
        "    # Use requests.get() to download the content\n",
        "    response = requests.get(url, stream=True) # Use stream=True for large files\n",
        "    response.raise_for_status()  # Raise an HTTPError for bad responses (4xx or 5xx)\n",
        "\n",
        "    # Open the specified filename in write-binary mode ('wb')\n",
        "    with open(filename, 'wb') as f:\n",
        "        for chunk in response.iter_content(chunk_size=8192):\n",
        "            f.write(chunk)\n",
        "\n",
        "    # Print a confirmation message\n",
        "    print(f\"Successfully downloaded and saved Shakespeare's complete works to '{filename}'.\")\n",
        "except requests.exceptions.RequestException as e:\n",
        "    print(f\"Error downloading the file: {e}\")\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attempting to download from: https://www.gutenberg.org/files/100/100-0.txt\n",
            "Successfully downloaded and saved Shakespeare's complete works to 'shakespeare.txt'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ff8c1c2"
      },
      "source": [
        "## Build Vocabulary and Tokenizer\n",
        "\n",
        "### Subtask:\n",
        "Process the downloaded Shakespeare text to build a vocabulary of unique words/tokens and assign numerical IDs to them. This will also involve creating a function to tokenize raw text into sequences of these numerical IDs.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "865cc4f6"
      },
      "source": [
        "**Reasoning**:\n",
        "I need to read the content of the `shakespeare.txt` file, convert it to lowercase, remove punctuation, and split it into words to begin building the vocabulary. I'll use the `re` module for punctuation removal.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ac74641f",
        "outputId": "be877897-cd61-44ed-bbac-c22ee0b904e7"
      },
      "source": [
        "import re\n",
        "from collections import Counter\n",
        "\n",
        "# 1. Read the content of the shakespeare.txt file\n",
        "filename = 'shakespeare.txt'\n",
        "\n",
        "with open(filename, 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "# 2. Preprocess the text\n",
        "# Convert to lowercase\n",
        "text = text.lower()\n",
        "\n",
        "# Remove punctuation and digits, replace with space, then split into words\n",
        "# Keep ' and ’ as part of contractions, but remove other non-alphanumeric chars\n",
        "text = re.sub(r'[^a-z\\s’\\[\\]]', ' ', text) # Corrected regex: removed explicit \\n and added [] around ’\n",
        "text = re.sub(r'\\s+', ' ', text).strip() # Replace multiple spaces with a single space\n",
        "\n",
        "words = text.split()\n",
        "\n",
        "print(f\"Total words after initial preprocessing: {len(words)}\")\n",
        "print(f\"First 20 words: {words[:20]}\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total words after initial preprocessing: 976881\n",
            "First 20 words: ['start', 'of', 'the', 'project', 'gutenberg', 'ebook', 'the', 'complete', 'works', 'of', 'william', 'shakespeare', 'by', 'william', 'shakespeare', 'contents', 'the', 'sonnets', 'all’s', 'well']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c4163877"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that the text has been preprocessed into a list of words, I need to create a vocabulary of unique words, assign them numerical IDs, and define a `tokenize` function that uses this vocabulary, handling unknown words and padding tokens as instructed.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "36a26461",
        "outputId": "40595ced-2471-4369-8a7a-b3b6cc82be3c"
      },
      "source": [
        "from collections import Counter\n",
        "\n",
        "# 3. Create a vocabulary by collecting all unique words\n",
        "word_counts = Counter(words)\n",
        "# Limit vocabulary size to manage memory and model complexity\n",
        "vocab_size_limit = 10000 # Example limit\n",
        "most_common_words = [word for word, _ in word_counts.most_common(vocab_size_limit - 2)] # -2 for <unk> and <pad>\n",
        "\n",
        "# 4. Assign unique integer IDs to each word\n",
        "vocabulary = {\"<pad>\": 0, \"<unk>\": 1}\n",
        "for i, word in enumerate(most_common_words):\n",
        "    vocabulary[word] = i + 2  # Start IDs from 2, after <pad> and <unk>\n",
        "\n",
        "# Reverse mapping for demonstration (optional)\n",
        "id_to_word = {v: k for k, v in vocabulary.items()}\n",
        "\n",
        "print(f\"Vocabulary size: {len(vocabulary)}\")\n",
        "print(f\"First 10 vocabulary items: {list(vocabulary.items())[:10]}\")\n",
        "\n",
        "# 5. Define a tokenize function\n",
        "def tokenize(text_string, vocab, max_seq_len=None):\n",
        "    # Preprocess the input text string similar to how the main text was processed\n",
        "    text_string = text_string.lower()\n",
        "    text_string = re.sub(r'[^a-z\\s’\\[\\]]', ' ', text_string)\n",
        "    text_string = re.sub(r'\\s+', ' ', text_string).strip()\n",
        "\n",
        "    tokens = text_string.split()\n",
        "    token_ids = [vocab.get(token, vocab[\"<unk>\"]) for token in tokens]\n",
        "\n",
        "    # Apply padding if max_seq_len is specified\n",
        "    if max_seq_len is not None:\n",
        "        if len(token_ids) > max_seq_len:\n",
        "            token_ids = token_ids[:max_seq_len]\n",
        "        elif len(token_ids) < max_seq_len:\n",
        "            token_ids = token_ids + [vocab[\"<pad>\"]] * (max_seq_len - len(token_ids))\n",
        "\n",
        "    return token_ids\n",
        "\n",
        "# Demonstrate the tokenizer\n",
        "sample_text = \"To be, or not to be, that is the question: Whether 'tis nobler in the mind to suffer\" # Example from Hamlet\n",
        "token_ids_sample = tokenize(sample_text, vocabulary)\n",
        "print(f\"\\nSample text for tokenization: {sample_text}\")\n",
        "print(f\"Tokenized IDs (first 20): {token_ids_sample[:20]}\")\n",
        "print(f\"Reconstructed text (first 20): {' '.join([id_to_word.get(idx, '<unk>') for idx in token_ids_sample[:20]])}\")\n",
        "\n",
        "# Demonstrate with padding\n",
        "token_ids_padded = tokenize(\"A rose by any other name\", vocabulary, max_seq_len=10)\n",
        "print(f\"\\nSample text for padded tokenization: 'A rose by any other name' (max_seq_len=10)\")\n",
        "print(f\"Tokenized IDs (padded): {token_ids_padded}\")\n",
        "print(f\"Reconstructed text (padded): {' '.join([id_to_word.get(idx, '<unk>') for idx in token_ids_padded])}\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size: 10000\n",
            "First 10 vocabulary items: [('<pad>', 0), ('<unk>', 1), ('the', 2), ('and', 3), ('i', 4), ('to', 5), ('of', 6), ('a', 7), ('you', 8), ('my', 9)]\n",
            "\n",
            "Sample text for tokenization: To be, or not to be, that is the question: Whether 'tis nobler in the mind to suffer\n",
            "Tokenized IDs (first 20): [5, 19, 56, 13, 5, 19, 11, 12, 2, 722, 961, 1, 3589, 10, 2, 289, 5, 1181]\n",
            "Reconstructed text (first 20): to be or not to be that is the question whether <unk> nobler in the mind to suffer\n",
            "\n",
            "Sample text for padded tokenization: 'A rose by any other name' (max_seq_len=10)\n",
            "Tokenized IDs (padded): [7, 1178, 36, 160, 163, 170, 0, 0, 0, 0]\n",
            "Reconstructed text (padded): a rose by any other name <pad> <pad> <pad> <pad>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7132ac46"
      },
      "source": [
        "## Create Shakespeare Dataset Class\n",
        "\n",
        "### Subtask:\n",
        "Develop a `ShakespeareDataset` class that inherits from `torch.utils.data.Dataset`. This class will load the tokenized Shakespeare text, prepare sequences of a specified length, and serve them to the DataLoader.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bda806a2"
      },
      "source": [
        "**Reasoning**:\n",
        "I need to define the `ShakespeareDataset` class as specified in the instructions. This involves implementing the `__init__`, `__len__`, and `__getitem__` methods to handle tokenized text, sequence length, and tensor conversion.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f88c0258",
        "outputId": "9e5daf20-0780-4e63-da4d-d268fa592d3a"
      },
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "# Ensure tokenize and vocabulary are available from previous cells\n",
        "# (Assuming they are already defined in the notebook scope)\n",
        "\n",
        "class ShakespeareDataset(Dataset):\n",
        "    def __init__(self, text_filepath: str, vocab: dict, seq_len: int):\n",
        "        self.vocab = vocab\n",
        "        self.seq_len = seq_len\n",
        "\n",
        "        # Read and tokenize the entire text\n",
        "        with open(text_filepath, 'r', encoding='utf-8') as f:\n",
        "            raw_text = f.read()\n",
        "\n",
        "        self.tokenized_text = tokenize(raw_text, self.vocab) # Use the global tokenize function\n",
        "\n",
        "        # Filter out potential padding tokens from the end if the last sequence would just be padding\n",
        "        # This ensures meaningful sequences for training\n",
        "        # A more robust approach might be to handle <pad> tokens within the model's loss mask if they appear mid-sequence.\n",
        "        # For this dataset, we assume we want to learn from actual text.\n",
        "        # If the original raw_text was very short, tokenized_text might be all <pad> if max_seq_len was used during tokenize.\n",
        "        # For this setup, `tokenize` is called without `max_seq_len`, so `tokenized_text` is just the sequence of real tokens.\n",
        "\n",
        "        # The number of possible input sequences of length `seq_len`\n",
        "        # For a sequence 'A B C D', with seq_len=2:\n",
        "        # Input: 'A B', Target: 'B C' (idx 0)\n",
        "        # Input: 'B C', Target: 'C D' (idx 1)\n",
        "        # So, the last possible `idx` is len(tokenized_text) - seq_len - 1.\n",
        "        # Thus, total number of sequences is (len(tokenized_text) - seq_len - 1) + 1 = len(tokenized_text) - seq_len\n",
        "        # We need at least `seq_len + 1` tokens to form one (input, target) pair.\n",
        "        if len(self.tokenized_text) < self.seq_len + 1:\n",
        "            raise ValueError(\"Text is too short for the specified sequence length.\")\n",
        "\n",
        "\n",
        "        print(f\"ShakespeareDataset initialized:\")\n",
        "        print(f\"  Total tokens: {len(self.tokenized_text)}\")\n",
        "        print(f\"  Sequence length: {self.seq_len}\")\n",
        "        print(f\"  Number of available sequences: {len(self)}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        # We need seq_len tokens for input and 1 token for target, so seq_len + 1 tokens total for each sample.\n",
        "        # If tokenized_text has N tokens, we can form N - seq_len samples.\n",
        "        # For example, if N=5, seq_len=2:\n",
        "        #   [t0, t1, t2, t3, t4]\n",
        "        #   idx 0: input [t0,t1], target [t1,t2]\n",
        "        #   idx 1: input [t1,t2], target [t2,t3]\n",
        "        #   idx 2: input [t2,t3], target [t3,t4]\n",
        "        # There are 3 samples. N - seq_len = 5 - 2 = 3.\n",
        "        return len(self.tokenized_text) - self.seq_len\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Input sequence: tokens from `idx` to `idx + seq_len`\n",
        "        input_sequence = self.tokenized_text[idx : idx + self.seq_len]\n",
        "\n",
        "        # Target sequence: tokens from `idx + 1` to `idx + seq_len + 1`\n",
        "        target_sequence = self.tokenized_text[idx + 1 : idx + self.seq_len + 1]\n",
        "\n",
        "        # Convert to torch.Tensor\n",
        "        input_tensor = torch.tensor(input_sequence, dtype=torch.long)\n",
        "        target_tensor = torch.tensor(target_sequence, dtype=torch.long)\n",
        "\n",
        "        return input_tensor, target_tensor\n",
        "\n",
        "print(\"ShakespeareDataset class defined.\")\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ShakespeareDataset class defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3bc6f6d1"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that the `ShakespeareDataset` class is defined, I will create an instance of it using the `shakespeare.txt` file, the previously created `vocabulary`, and a specified `seq_len`. Then, I will retrieve an example item to verify that the `__getitem__` method correctly returns input and target tensors.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "60ee87e4",
        "outputId": "3f26fa00-1874-4f2d-8a2a-f987768a975e"
      },
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Define parameters for the dataset\n",
        "TEXT_FILEPATH = 'shakespeare.txt'\n",
        "SEQUENCE_LENGTH = 64 # This should match the expected seq_len in the model\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "# Instantiate the ShakespeareDataset\n",
        "try:\n",
        "    shakespeare_dataset = ShakespeareDataset(TEXT_FILEPATH, vocabulary, SEQUENCE_LENGTH)\n",
        "\n",
        "    # Create a DataLoader to iterate over the dataset\n",
        "    shakespeare_dataloader = DataLoader(shakespeare_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "    print(f\"\\nDataset and DataLoader created successfully.\")\n",
        "    print(f\"Number of batches in DataLoader: {len(shakespeare_dataloader)}\")\n",
        "\n",
        "    # Get one batch from the DataLoader to inspect the output\n",
        "    for i, (inputs, targets) in enumerate(shakespeare_dataloader):\n",
        "        print(f\"\\nExample batch from DataLoader (Batch {i+1}):\")\n",
        "        print(f\"Input batch shape: {inputs.shape}\")\n",
        "        print(f\"Target batch shape: {targets.shape}\")\n",
        "        print(f\"First input sequence in batch: {inputs[0].tolist()}\")\n",
        "        print(f\"First target sequence in batch: {targets[0].tolist()}\")\n",
        "        break # Only print the first batch\n",
        "\n",
        "    # Verify the mapping to words for the first sequence\n",
        "    print(\"\\nFirst input sequence (reconstructed words):\")\n",
        "    print(' '.join([id_to_word.get(idx.item(), '<unk>') for idx in inputs[0]]))\n",
        "    print(\"\\nFirst target sequence (reconstructed words):\")\n",
        "    print(' '.join([id_to_word.get(idx.item(), '<unk>') for idx in targets[0]]))\n",
        "\n",
        "except ValueError as e:\n",
        "    print(f\"Error creating dataset: {e}\")\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ShakespeareDataset initialized:\n",
            "  Total tokens: 976881\n",
            "  Sequence length: 64\n",
            "  Number of available sequences: 976817\n",
            "\n",
            "Dataset and DataLoader created successfully.\n",
            "Number of batches in DataLoader: 30526\n",
            "\n",
            "Example batch from DataLoader (Batch 1):\n",
            "Input batch shape: torch.Size([32, 64])\n",
            "Target batch shape: torch.Size([32, 64])\n",
            "First input sequence in batch: [1529, 13, 7, 78, 39, 751, 18, 3602, 56, 1611, 2, 2383, 6, 1, 435, 10, 21, 5505, 3294, 23, 39, 19, 1, 5, 21, 1256, 2153, 54, 7182, 287, 167, 98, 110, 2306, 1624, 1606, 3317, 3, 226, 21, 265, 35, 2, 1479, 3317, 34, 59, 7, 410, 410, 9, 181, 525, 360, 12, 200, 9506, 81, 2, 142, 6227, 288, 125, 384]\n",
            "First target sequence in batch: [13, 7, 78, 39, 751, 18, 3602, 56, 1611, 2, 2383, 6, 1, 435, 10, 21, 5505, 3294, 23, 39, 19, 1, 5, 21, 1256, 2153, 54, 7182, 287, 167, 98, 110, 2306, 1624, 1606, 3317, 3, 226, 21, 265, 35, 2, 1479, 3317, 34, 59, 7, 410, 410, 9, 181, 525, 360, 12, 200, 9506, 81, 2, 142, 6227, 288, 125, 384, 3]\n",
            "\n",
            "First input sequence (reconstructed words):\n",
            "meaning not a man shall pass his quarter or offend the stream of <unk> justice in your city’s bounds but shall be <unk> to your public laws at heaviest answer both ’tis most nobly spoken alcibiades descend and keep your words [ the senators descend ] enter a soldier soldier my noble general timon is dead entombed upon the very hem o’ th’ sea\n",
            "\n",
            "First target sequence (reconstructed words):\n",
            "not a man shall pass his quarter or offend the stream of <unk> justice in your city’s bounds but shall be <unk> to your public laws at heaviest answer both ’tis most nobly spoken alcibiades descend and keep your words [ the senators descend ] enter a soldier soldier my noble general timon is dead entombed upon the very hem o’ th’ sea and\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7f326185"
      },
      "source": [
        "## Update Main Training Loop\n",
        "\n",
        "### Subtask:\n",
        "Modify the `main` function to use the newly created `ShakespeareDataset` and adjust the `vocab_size` hyperparameter based on the vocabulary built from the Shakespeare text. This will ensure the model is trained on realistic textual data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "41ea0278"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask is to modify the `main` function and the `train_step` function to use the `ShakespeareDataset` and the correct `vocab_size`. The provided code block already contains these modifications. Executing this cell will apply these changes, fulfilling the subtask requirements.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7cec1c9e"
      },
      "source": [
        "def main():\n",
        "    # Hyperparameters\n",
        "    # Update vocab_size to reflect the actual size of the Shakespeare vocabulary\n",
        "    vocab_size = len(vocabulary)\n",
        "    embed_dim = 256\n",
        "    num_layers = 4\n",
        "    num_heads = 4\n",
        "    num_experts = 8\n",
        "    num_shared_experts = 2\n",
        "    top_k = 2\n",
        "    hidden_dim = 512\n",
        "    batch_size = BATCH_SIZE # Use the globally defined BATCH_SIZE\n",
        "    num_epochs = 3\n",
        "    learning_rate = 3e-4\n",
        "\n",
        "    # Create model\n",
        "    model = MoEModel(\n",
        "        vocab_size=vocab_size,\n",
        "        embed_dim=embed_dim,\n",
        "        num_layers=num_layers,\n",
        "        num_heads=num_heads,\n",
        "        num_experts=num_experts,\n",
        "        num_shared_experts=num_shared_experts,\n",
        "        top_k=top_k,\n",
        "        hidden_dim=hidden_dim\n",
        "    )\n",
        "\n",
        "    # Setup multi-GPU if available\n",
        "    model = setup_distributed_model(model, device_count)\n",
        "    model = model.to(device)\n",
        "\n",
        "    # Print model info\n",
        "    num_params = sum(p.numel() for p in model.parameters())\n",
        "    print(f\"\\nModel Parameters: {num_params:,}\")\n",
        "    print(f\"Number of Experts: {num_experts}\")\n",
        "    print(f\"Number of Shared Experts: {num_shared_experts}\")\n",
        "    print(f\"Top-K Experts: {top_k}\")\n",
        "\n",
        "    # Create dataset and dataloader\n",
        "    # Use the ShakespeareDataset instead of SimpleTextDataset\n",
        "    # Ensure TEXT_FILEPATH, vocabulary, and SEQUENCE_LENGTH are accessible (they are global)\n",
        "    dataset = ShakespeareDataset(TEXT_FILEPATH, vocabulary, SEQUENCE_LENGTH)\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    # Optimizer\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    # Training loop\n",
        "    print(\"\\nStarting training...\")\n",
        "    for epoch in range(num_epochs):\n",
        "        epoch_losses = {'total': 0, 'main': 0, 'aux': 0}\n",
        "\n",
        "        for i, (inputs, targets) in enumerate(dataloader):\n",
        "            losses = train_step(model, inputs, targets, optimizer, device)\n",
        "\n",
        "            epoch_losses['total'] += losses['total_loss']\n",
        "            epoch_losses['main'] += losses['main_loss']\n",
        "            epoch_losses['aux'] += losses['aux_loss']\n",
        "\n",
        "            if (i + 1) % 10 == 0:\n",
        "                print(f\"Epoch {epoch+1}/{num_epochs}, Step {i+1}/{len(dataloader)}, \"\n",
        "                      f\"Loss: {losses['total_loss']:.4f} \"\n",
        "                      f\"(Main: {losses['main_loss']:.4f}, Aux: {losses['aux_loss']:.4f})\")\n",
        "\n",
        "        avg_losses = {k: v / len(dataloader) for k, v in epoch_losses.items()}\n",
        "        print(f\"\\nEpoch {epoch+1} Summary - \"\n",
        "              f\"Avg Loss: {avg_losses['total']:.4f} \"\n",
        "              f\"(Main: {avg_losses['main']:.4f}, Aux: {avg_losses['aux']:.4f})\\n\")\n",
        "\n",
        "    print(\"Training complete!\")\n",
        "\n",
        "    return model\n",
        "\n",
        "# ============================================================================\n",
        "# TRAINING UTILITIES (MODIFIED FOR SHAKESPEARE DATASET)\n",
        "# ============================================================================\n",
        "\n",
        "def train_step(model, inputs, targets, optimizer, device):\n",
        "    \"\"\"Single training step, adapted for direct inputs and targets\"\"\"\n",
        "    model.train()\n",
        "\n",
        "    # Move to device\n",
        "    inputs = inputs.to(device)\n",
        "    targets = targets.to(device)\n",
        "\n",
        "    # Forward pass\n",
        "    logits, aux_loss = model(inputs)\n",
        "\n",
        "    # Compute main loss (cross-entropy)\n",
        "    main_loss = F.cross_entropy(\n",
        "        logits.reshape(-1, logits.size(-1)),\n",
        "        targets.reshape(-1)\n",
        "    )\n",
        "\n",
        "    # Total loss includes auxiliary losses\n",
        "    total_loss = main_loss + aux_loss\n",
        "\n",
        "    # Backward pass\n",
        "    optimizer.zero_grad()\n",
        "    total_loss.backward()\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "    optimizer.step()\n",
        "\n",
        "    return {\n",
        "        'total_loss': total_loss.item(),\n",
        "        'main_loss': main_loss.item(),\n",
        "        'aux_loss': aux_loss.item()\n",
        "    }\n",
        "\n",
        "# ============================================================================\n",
        "# MAIN TRAINING LOOP (MODIFIED)\n",
        "# ============================================================================\n",
        "\n",
        "def main():\n",
        "    # Hyperparameters\n",
        "    # Update vocab_size to reflect the actual size of the Shakespeare vocabulary\n",
        "    vocab_size = len(vocabulary)\n",
        "    embed_dim = 256\n",
        "    num_layers = 4\n",
        "    num_heads = 4\n",
        "    num_experts = 8\n",
        "    num_shared_experts = 2\n",
        "    top_k = 2\n",
        "    hidden_dim = 512\n",
        "    batch_size = BATCH_SIZE # Use the globally defined BATCH_SIZE\n",
        "    num_epochs = 3\n",
        "    learning_rate = 3e-4\n",
        "\n",
        "    # Create model\n",
        "    model = MoEModel(\n",
        "        vocab_size=vocab_size,\n",
        "        embed_dim=embed_dim,\n",
        "        num_layers=num_layers,\n",
        "        num_heads=num_heads,\n",
        "        num_experts=num_experts,\n",
        "        num_shared_experts=num_shared_experts,\n",
        "        top_k=top_k,\n",
        "        hidden_dim=hidden_dim\n",
        "    )\n",
        "\n",
        "    # Setup multi-GPU if available\n",
        "    model = setup_distributed_model(model, device_count)\n",
        "    model = model.to(device)\n",
        "\n",
        "    # Print model info\n",
        "    num_params = sum(p.numel() for p in model.parameters())\n",
        "    print(f\"\\nModel Parameters: {num_params:,}\")\n",
        "    print(f\"Number of Experts: {num_experts}\")\n",
        "    print(f\"Number of Shared Experts: {num_shared_experts}\")\n",
        "    print(f\"Top-K Experts: {top_k}\")\n",
        "\n",
        "    # Create dataset and dataloader\n",
        "    # Use the ShakespeareDataset instead of SimpleTextDataset\n",
        "    # Ensure TEXT_FILEPATH, vocabulary, and SEQUENCE_LENGTH are accessible (they are global)\n",
        "    dataset = ShakespeareDataset(TEXT_FILEPATH, vocabulary, SEQUENCE_LENGTH)\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    # Optimizer\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    # Training loop\n",
        "    print(\"\\nStarting training...\")\n",
        "    for epoch in range(num_epochs):\n",
        "        epoch_losses = {'total': 0, 'main': 0, 'aux': 0}\n",
        "\n",
        "        for i, (inputs, targets) in enumerate(dataloader):\n",
        "            losses = train_step(model, inputs, targets, optimizer, device)\n",
        "\n",
        "            epoch_losses['total'] += losses['total_loss']\n",
        "            epoch_losses['main'] += losses['main_loss']\n",
        "            epoch_losses['aux'] += losses['aux_loss']\n",
        "\n",
        "            if (i + 1) % 10 == 0:\n",
        "                print(f\"Epoch {epoch+1}/{num_epochs}, Step {i+1}/{len(dataloader)}, \"\n",
        "                      f\"Loss: {losses['total_loss']:.4f} \"\n",
        "                      f\"(Main: {losses['main_loss']:.4f}, Aux: {losses['aux_loss']:.4f})\")\n",
        "\n",
        "        avg_losses = {k: v / len(dataloader) for k, v in epoch_losses.items()}\n",
        "        print(f\"\\nEpoch {epoch+1} Summary - \"\n",
        "              f\"Avg Loss: {avg_losses['total']:.4f} \"\n",
        "              f\"(Main: {avg_losses['main']:.4f}, Aux: {avg_losses['aux']:.4f})\\n\")\n",
        "\n",
        "    print(\"Training complete!\")\n",
        "\n",
        "    return model"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e082c84a"
      },
      "source": [
        "# Task\n",
        "The original user task is to modify the `ShakespeareDataset` class to incorporate a `data_fraction` parameter, allowing the MoE model to be trained on a subset of the tokenized Shakespeare text."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5dfece66"
      },
      "source": [
        "## Create Shakespeare Dataset Class\n",
        "\n",
        "### Subtask:\n",
        "Develop a `ShakespeareDataset` class that inherits from `torch.utils.data.Dataset`. This class will load the tokenized Shakespeare text, prepare sequences of a specified length, and serve them to the DataLoader. This class will be modified to include a `data_fraction` parameter to use only a subset of the total tokenized text.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9d9ebaea"
      },
      "source": [
        "**Reasoning**:\n",
        "I need to modify the existing `ShakespeareDataset` class in the notebook by adding a `data_fraction` parameter to its `__init__` method, using it to select a subset of the tokenized text, and updating the print statements to reflect this change. This requires rewriting the `ShakespeareDataset` class definition.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6aad2a1d",
        "outputId": "d1f6b594-5fa9-4a2f-f291-0404722527db"
      },
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "# Ensure tokenize and vocabulary are available from previous cells\n",
        "# (Assuming they are already defined in the notebook scope)\n",
        "\n",
        "class ShakespeareDataset(Dataset):\n",
        "    def __init__(self, text_filepath: str, vocab: dict, seq_len: int, data_fraction: float = 1.0):\n",
        "        self.vocab = vocab\n",
        "        self.seq_len = seq_len\n",
        "        self.data_fraction = data_fraction\n",
        "\n",
        "        # Read and tokenize the entire text\n",
        "        with open(text_filepath, 'r', encoding='utf-8') as f:\n",
        "            raw_text = f.read()\n",
        "\n",
        "        full_tokenized_text = tokenize(raw_text, self.vocab) # Use the global tokenize function\n",
        "\n",
        "        # Calculate number of tokens to use based on data_fraction\n",
        "        num_tokens_to_use = int(len(full_tokenized_text) * self.data_fraction)\n",
        "        self.tokenized_text = full_tokenized_text[:num_tokens_to_use]\n",
        "\n",
        "        # The number of possible input sequences of length `seq_len`\n",
        "        if len(self.tokenized_text) < self.seq_len + 1:\n",
        "            raise ValueError(\"Text is too short for the specified sequence length after applying data_fraction.\")\n",
        "\n",
        "        print(f\"ShakespeareDataset initialized:\")\n",
        "        print(f\"  Using data fraction: {self.data_fraction:.2f}\")\n",
        "        print(f\"  Total tokens (after fraction): {len(self.tokenized_text)}\")\n",
        "        print(f\"  Sequence length: {self.seq_len}\")\n",
        "        print(f\"  Number of available sequences: {len(self)}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        # We need seq_len tokens for input and 1 token for target, so seq_len + 1 tokens total for each sample.\n",
        "        # If tokenized_text has N tokens, we can form N - seq_len samples.\n",
        "        return len(self.tokenized_text) - self.seq_len\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Input sequence: tokens from `idx` to `idx + seq_len`\n",
        "        input_sequence = self.tokenized_text[idx : idx + self.seq_len]\n",
        "\n",
        "        # Target sequence: tokens from `idx + 1` to `idx + seq_len + 1`\n",
        "        target_sequence = self.tokenized_text[idx + 1 : idx + self.seq_len + 1]\n",
        "\n",
        "        # Convert to torch.Tensor\n",
        "        input_tensor = torch.tensor(input_sequence, dtype=torch.long)\n",
        "        target_tensor = torch.tensor(target_sequence, dtype=torch.long)\n",
        "\n",
        "        return input_tensor, target_tensor\n",
        "\n",
        "print(\"ShakespeareDataset class defined with data_fraction support.\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ShakespeareDataset class defined with data_fraction support.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "203de2fb"
      },
      "source": [
        "## Update Main Training Loop\n",
        "\n",
        "### Subtask:\n",
        "Modify the `main` function to use the updated `ShakespeareDataset` with the `data_fraction` parameter, and adjust the `vocab_size` hyperparameter based on the vocabulary built from the Shakespeare text. This will ensure the model is trained on a limited, realistic textual data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a467786d"
      },
      "source": [
        "**Reasoning**:\n",
        "I need to modify the `main` function to incorporate the `data_fraction` hyperparameter and pass it to the `ShakespeareDataset` constructor, as well as ensure the `vocab_size` is correctly set from the global `vocabulary`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "190a5409"
      },
      "source": [
        "def main():\n",
        "    # Hyperparameters\n",
        "    # Update vocab_size to reflect the actual size of the Shakespeare vocabulary\n",
        "    vocab_size = len(vocabulary)\n",
        "    embed_dim = 256\n",
        "    num_layers = 4\n",
        "    num_heads = 4\n",
        "    num_experts = 8\n",
        "    num_shared_experts = 2\n",
        "    top_k = 2\n",
        "    hidden_dim = 512\n",
        "    batch_size = BATCH_SIZE # Use the globally defined BATCH_SIZE\n",
        "    num_epochs = 3\n",
        "    learning_rate = 3e-4\n",
        "    data_fraction = 0.1 # New hyperparameter for data fraction\n",
        "\n",
        "    # Create model\n",
        "    model = MoEModel(\n",
        "        vocab_size=vocab_size,\n",
        "        embed_dim=embed_dim,\n",
        "        num_layers=num_layers,\n",
        "        num_heads=num_heads,\n",
        "        num_experts=num_experts,\n",
        "        num_shared_experts=num_shared_experts,\n",
        "        top_k=top_k,\n",
        "        hidden_dim=hidden_dim\n",
        "    )\n",
        "\n",
        "    # Setup multi-GPU if available\n",
        "    model = setup_distributed_model(model, device_count)\n",
        "    model = model.to(device)\n",
        "\n",
        "    # Print model info\n",
        "    num_params = sum(p.numel() for p in model.parameters())\n",
        "    print(f\"\\nModel Parameters: {num_params:,}\")\n",
        "    print(f\"Number of Experts: {num_experts}\")\n",
        "    print(f\"Number of Shared Experts: {num_shared_experts}\")\n",
        "    print(f\"Top-K Experts: {top_k}\")\n",
        "\n",
        "    # Create dataset and dataloader\n",
        "    # Use the ShakespeareDataset instead of SimpleTextDataset\n",
        "    # Ensure TEXT_FILEPATH, vocabulary, and SEQUENCE_LENGTH are accessible (they are global)\n",
        "    dataset = ShakespeareDataset(TEXT_FILEPATH, vocabulary, SEQUENCE_LENGTH, data_fraction=data_fraction)\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    # Optimizer\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    # Training loop\n",
        "    print(\"\\nStarting training...\")\n",
        "    for epoch in range(num_epochs):\n",
        "        epoch_losses = {'total': 0, 'main': 0, 'aux': 0}\n",
        "\n",
        "        for i, (inputs, targets) in enumerate(dataloader):\n",
        "            losses = train_step(model, inputs, targets, optimizer, device)\n",
        "\n",
        "            epoch_losses['total'] += losses['total_loss']\n",
        "            epoch_losses['main'] += losses['main_loss']\n",
        "            epoch_losses['aux'] += losses['aux_loss']\n",
        "\n",
        "            if (i + 1) % 10 == 0:\n",
        "                print(f\"Epoch {epoch+1}/{num_epochs}, Step {i+1}/{len(dataloader)}, \"\n",
        "                      f\"Loss: {losses['total_loss']:.4f} \"\n",
        "                      f\"(Main: {losses['main_loss']:.4f}, Aux: {losses['aux_loss']:.4f})\")\n",
        "\n",
        "        avg_losses = {k: v / len(dataloader) for k, v in epoch_losses.items()}\n",
        "        print(f\"\\nEpoch {epoch+1} Summary - \"\n",
        "              f\"Avg Loss: {avg_losses['total']:.4f} \"\n",
        "              f\"(Main: {avg_losses['main']:.4f}, Aux: {avg_losses['aux']:.4f})\\n\")\n",
        "\n",
        "    print(\"Training complete!\")\n",
        "\n",
        "    return model\n",
        "\n",
        "# ============================================================================\n",
        "# TRAINING UTILITIES (MODIFIED FOR SHAKESPEARE DATASET)\n",
        "# ============================================================================\n",
        "\n",
        "def train_step(model, inputs, targets, optimizer, device):\n",
        "    \"\"\"Single training step, adapted for direct inputs and targets\"\"\"\n",
        "    model.train()\n",
        "\n",
        "    # Move to device\n",
        "    inputs = inputs.to(device)\n",
        "    targets = targets.to(device)\n",
        "\n",
        "    # Forward pass\n",
        "    logits, aux_loss = model(inputs)\n",
        "\n",
        "    # Compute main loss (cross-entropy)\n",
        "    main_loss = F.cross_entropy(\n",
        "        logits.reshape(-1, logits.size(-1)),\n",
        "        targets.reshape(-1)\n",
        "    )\n",
        "\n",
        "    # Total loss includes auxiliary losses\n",
        "    total_loss = main_loss + aux_loss\n",
        "\n",
        "    # Backward pass\n",
        "    optimizer.zero_grad()\n",
        "    total_loss.backward()\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "    optimizer.step()\n",
        "\n",
        "    return {\n",
        "        'total_loss': total_loss.item(),\n",
        "        'main_loss': main_loss.item(),\n",
        "        'aux_loss': aux_loss.item()\n",
        "    }\n",
        "\n",
        "# ============================================================================\n",
        "# MAIN TRAINING LOOP (MODIFIED)\n",
        "# ============================================================================\n",
        "\n",
        "def main():\n",
        "    # Hyperparameters\n",
        "    # Update vocab_size to reflect the actual size of the Shakespeare vocabulary\n",
        "    vocab_size = len(vocabulary)\n",
        "    embed_dim = 256\n",
        "    num_layers = 4\n",
        "    num_heads = 4\n",
        "    num_experts = 8\n",
        "    num_shared_experts = 2\n",
        "    top_k = 2\n",
        "    hidden_dim = 512\n",
        "    batch_size = BATCH_SIZE # Use the globally defined BATCH_SIZE\n",
        "    num_epochs = 3\n",
        "    learning_rate = 3e-4\n",
        "    data_fraction = 0.001 # New hyperparameter for data fraction\n",
        "\n",
        "    # Create model\n",
        "    model = MoEModel(\n",
        "        vocab_size=vocab_size,\n",
        "        embed_dim=embed_dim,\n",
        "        num_layers=num_layers,\n",
        "        num_heads=num_heads,\n",
        "        num_experts=num_experts,\n",
        "        num_shared_experts=num_shared_experts,\n",
        "        top_k=top_k,\n",
        "        hidden_dim=hidden_dim\n",
        "    )\n",
        "\n",
        "    # Setup multi-GPU if available\n",
        "    model = setup_distributed_model(model, device_count)\n",
        "    model = model.to(device)\n",
        "\n",
        "    # Print model info\n",
        "    num_params = sum(p.numel() for p in model.parameters())\n",
        "    print(f\"\\nModel Parameters: {num_params:,}\")\n",
        "    print(f\"Number of Experts: {num_experts}\")\n",
        "    print(f\"Number of Shared Experts: {num_shared_experts}\")\n",
        "    print(f\"Top-K Experts: {top_k}\")\n",
        "\n",
        "    # Create dataset and dataloader\n",
        "    # Use the ShakespeareDataset instead of SimpleTextDataset\n",
        "    # Ensure TEXT_FILEPATH, vocabulary, and SEQUENCE_LENGTH are accessible (they are global)\n",
        "    dataset = ShakespeareDataset(TEXT_FILEPATH, vocabulary, SEQUENCE_LENGTH, data_fraction=data_fraction)\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    # Optimizer\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    # Training loop\n",
        "    print(\"\\nStarting training...\")\n",
        "    for epoch in range(num_epochs):\n",
        "        epoch_losses = {'total': 0, 'main': 0, 'aux': 0}\n",
        "\n",
        "        for i, (inputs, targets) in enumerate(dataloader):\n",
        "            losses = train_step(model, inputs, targets, optimizer, device)\n",
        "\n",
        "            epoch_losses['total'] += losses['total_loss']\n",
        "            epoch_losses['main'] += losses['main_loss']\n",
        "            epoch_losses['aux'] += losses['aux_loss']\n",
        "\n",
        "            if (i + 1) % 10 == 0:\n",
        "                print(f\"Epoch {epoch+1}/{num_epochs}, Step {i+1}/{len(dataloader)}, \"\n",
        "                      f\"Loss: {losses['total_loss']:.4f} \"\n",
        "                      f\"(Main: {losses['main_loss']:.4f}, Aux: {losses['aux_loss']:.4f})\")\n",
        "\n",
        "        avg_losses = {k: v / len(dataloader) for k, v in epoch_losses.items()}\n",
        "        print(f\"\\nEpoch {epoch+1} Summary - \"\n",
        "              f\"Avg Loss: {avg_losses['total']:.4f} \"\n",
        "              f\"(Main: {avg_losses['main']:.4f}, Aux: {avg_losses['aux']:.4f})\\n\")\n",
        "\n",
        "    print(\"Training complete!\")\n",
        "\n",
        "    return model"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e807b87b"
      },
      "source": [
        "## Final Task\n",
        "\n",
        "### Subtask:\n",
        "Summarize the changes made to incorporate Shakespeare text tokenization with a limited dataset and confirm the new training setup.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "06aeb3f3"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Q&A\n",
        "1.  **What changes were made to incorporate Shakespeare text tokenization with a limited dataset?**\n",
        "    The `ShakespeareDataset` class was modified to accept a `data_fraction` parameter during initialization, allowing it to load and use only a specified percentage of the full tokenized Shakespeare text. The `main` function was updated to define and pass this `data_fraction` to the dataset.\n",
        "\n",
        "2.  **How was the new training setup confirmed?**\n",
        "    The training setup was confirmed by modifying the `main` function to explicitly set `data_fraction = 0.1` and pass it to the `ShakespeareDataset` constructor. Additionally, the `vocab_size` hyperparameter for the MoE model was dynamically set using `len(vocabulary)`, ensuring it correctly reflects the actual size of the Shakespeare text vocabulary.\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "*   The `ShakespeareDataset` class was successfully modified to accept a `data_fraction` parameter (defaulting to 1.0), which limits the number of tokens used from the original text.\n",
        "*   The `__init__` method of the `ShakespeareDataset` now calculates `num_tokens_to_use` as `int(len(full_tokenized_text) * self.data_fraction)` and slices the `full_tokenized_text` accordingly.\n",
        "*   The `main` function was updated to include a `data_fraction` hyperparameter, set to `0.1`, which is then passed to the `ShakespeareDataset` constructor.\n",
        "*   The `vocab_size` hyperparameter in the `main` function is now dynamically set to `len(vocabulary)`, ensuring the model's vocabulary size matches the actual size of the processed Shakespeare text vocabulary.\n",
        "\n",
        "### Insights or Next Steps\n",
        "*   This setup enables efficient experimentation with training the Mixture-of-Experts (MoE) model on various subsets of the Shakespeare text, which can be crucial for understanding performance at different data scales or for faster prototyping.\n",
        "*   The modular design, where `data_fraction` is a configurable hyperparameter, allows for easy adjustment of dataset size without modifying the core data loading logic, improving the flexibility and reproducibility of experiments.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Mixture of Experts (MoE) Implementation - FIXED VERSION\n",
        "Supports 0, 1, or 2 GPUs with full technical requirements\n",
        "Fixed: Auxiliary loss now properly returns as scalar for backward pass\n",
        "\"\"\"\n",
        "\n",
        "# ============================================================================\n",
        "# INSTALLATION & IMPORTS\n",
        "# ============================================================================\n",
        "\n",
        "# Install required packages (uncomment if needed)\n",
        "# !pip install torch transformers datasets accelerate\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import math\n",
        "from typing import Tuple, Optional\n",
        "import numpy as np\n",
        "import re\n",
        "from collections import Counter\n",
        "import requests\n",
        "\n",
        "# Check GPU availability\n",
        "device_count = torch.cuda.device_count()\n",
        "print(f\"Available GPUs: {device_count}\")\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# ============================================================================\n",
        "# EXPERT NETWORK\n",
        "# ============================================================================\n",
        "\n",
        "class Expert(nn.Module):\n",
        "    \"\"\"Single expert feedforward network\"\"\"\n",
        "    def __init__(self, input_dim: int, hidden_dim: int, dropout: float = 0.1):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim, input_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# ROUTER WITH GATING MECHANISM\n",
        "# ============================================================================\n",
        "\n",
        "class Router(nn.Module):\n",
        "    \"\"\"Gating network that routes tokens to experts\"\"\"\n",
        "    def __init__(self, input_dim: int, num_experts: int):\n",
        "        super().__init__()\n",
        "        self.gate = nn.Linear(input_dim, num_experts, bias=False)\n",
        "        self.num_experts = num_experts\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: [batch_size, seq_len, input_dim]\n",
        "        logits = self.gate(x)  # [batch_size, seq_len, num_experts]\n",
        "        return logits\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# MIXTURE OF EXPERTS LAYER - FIXED\n",
        "# ============================================================================\n",
        "\n",
        "class MixtureOfExpertsLayer(nn.Module):\n",
        "    \"\"\"\n",
        "    Complete MoE layer with all technical requirements:\n",
        "    - Multiple expert networks\n",
        "    - Gating/routing mechanism\n",
        "    - Sparse activation (top-k)\n",
        "    - Shared experts\n",
        "    - Load balancing loss\n",
        "    - Router Z-loss\n",
        "    - Capacity constraints\n",
        "\n",
        "    FIXED: Returns aux_loss as part of forward pass instead of storing it\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_dim: int,\n",
        "        hidden_dim: int,\n",
        "        num_experts: int = 8,\n",
        "        num_shared_experts: int = 2,\n",
        "        top_k: int = 2,\n",
        "        capacity_factor: float = 1.25,\n",
        "        load_balance_weight: float = 0.01,\n",
        "        z_loss_weight: float = 0.001,\n",
        "        dropout: float = 0.1\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.input_dim = input_dim\n",
        "        self.num_experts = num_experts\n",
        "        self.num_shared_experts = num_shared_experts\n",
        "        self.top_k = top_k\n",
        "        self.capacity_factor = capacity_factor\n",
        "        self.load_balance_weight = load_balance_weight\n",
        "        self.z_loss_weight = z_loss_weight\n",
        "\n",
        "        # Sparse experts (specialized)\n",
        "        self.experts = nn.ModuleList([\n",
        "            Expert(input_dim, hidden_dim, dropout)\n",
        "            for _ in range(num_experts)\n",
        "        ])\n",
        "\n",
        "        # Shared experts (always active)\n",
        "        self.shared_experts = nn.ModuleList([\n",
        "            Expert(input_dim, hidden_dim, dropout)\n",
        "            for _ in range(num_shared_experts)\n",
        "        ])\n",
        "\n",
        "        # Router\n",
        "        self.router = Router(input_dim, num_experts)\n",
        "\n",
        "    def compute_load_balance_loss(\n",
        "        self,\n",
        "        router_probs: torch.Tensor,\n",
        "        expert_mask: torch.Tensor\n",
        "    ) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Load Balancing Loss: L_bal = N * sum(f_i * P_i)\n",
        "\n",
        "        Args:\n",
        "            router_probs: [batch_size, seq_len, num_experts] - router probabilities\n",
        "            expert_mask: [batch_size, seq_len, num_experts] - binary mask of selected experts\n",
        "        \"\"\"\n",
        "        # P_i: mean probability allocated to expert i\n",
        "        P = router_probs.mean(dim=[0, 1])  # [num_experts]\n",
        "\n",
        "        # f_i: fraction of tokens routed to expert i\n",
        "        f = expert_mask.float().mean(dim=[0, 1])  # [num_experts]\n",
        "\n",
        "        # L_bal = N * sum(f_i * P_i)\n",
        "        load_balance_loss = self.num_experts * torch.sum(f * P)\n",
        "\n",
        "        return load_balance_loss\n",
        "\n",
        "    def compute_router_z_loss(self, router_logits: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Router Z-Loss: L_z = (1/B) * sum(log(sum(e^x_i)))^2\n",
        "        Encourages router logits to stay close to zero for stability\n",
        "\n",
        "        Args:\n",
        "            router_logits: [batch_size, seq_len, num_experts]\n",
        "        \"\"\"\n",
        "        # Log-sum-exp for numerical stability\n",
        "        log_sum_exp = torch.logsumexp(router_logits, dim=-1)  # [batch_size, seq_len]\n",
        "\n",
        "        # L_z = mean of squared log-sum-exp\n",
        "        z_loss = torch.mean(log_sum_exp ** 2)\n",
        "\n",
        "        return z_loss\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: [batch_size, seq_len, input_dim]\n",
        "        Returns:\n",
        "            output: [batch_size, seq_len, input_dim]\n",
        "            aux_loss: scalar tensor (kept as 1D for DataParallel compatibility)\n",
        "        \"\"\"\n",
        "        batch_size, seq_len, input_dim = x.shape\n",
        "        num_tokens = batch_size * seq_len\n",
        "\n",
        "        # Flatten for routing\n",
        "        x_flat = x.view(num_tokens, input_dim)  # [num_tokens, input_dim]\n",
        "\n",
        "        # ====================================================================\n",
        "        # ROUTING: Compute router logits and probabilities\n",
        "        # ====================================================================\n",
        "        router_logits = self.router(x)  # [batch_size, seq_len, num_experts]\n",
        "        router_logits_flat = router_logits.view(num_tokens, self.num_experts)\n",
        "        router_probs = F.softmax(router_logits_flat, dim=-1)  # [num_tokens, num_experts]\n",
        "\n",
        "        # ====================================================================\n",
        "        # SPARSE ACTIVATION: Top-k expert selection\n",
        "        # ====================================================================\n",
        "        top_k_probs, top_k_indices = torch.topk(\n",
        "            router_probs,\n",
        "            self.top_k,\n",
        "            dim=-1\n",
        "        )  # Both: [num_tokens, top_k]\n",
        "\n",
        "        # Normalize top-k probabilities\n",
        "        top_k_probs = top_k_probs / top_k_probs.sum(dim=-1, keepdim=True)\n",
        "\n",
        "        # Create expert mask for load balancing\n",
        "        expert_mask = torch.zeros_like(router_probs)  # [num_tokens, num_experts]\n",
        "        expert_mask.scatter_(1, top_k_indices, 1.0)\n",
        "        expert_mask = expert_mask.view(batch_size, seq_len, self.num_experts)\n",
        "\n",
        "        # ====================================================================\n",
        "        # CAPACITY CONSTRAINTS: Limit tokens per expert\n",
        "        # ====================================================================\n",
        "        capacity = int(self.capacity_factor * num_tokens / self.num_experts)\n",
        "\n",
        "        # ====================================================================\n",
        "        # EXPERT COMPUTATION: OPTIMIZED BATCHED PROCESSING\n",
        "        # ====================================================================\n",
        "        # Initialize output\n",
        "        output = torch.zeros_like(x_flat)  # [num_tokens, input_dim]\n",
        "\n",
        "        # Process each expert in batch\n",
        "        for expert_idx in range(self.num_experts):\n",
        "            # Find which tokens are routed to this expert and at which top-k position\n",
        "            expert_mask_flat = (top_k_indices == expert_idx)  # [num_tokens, top_k]\n",
        "\n",
        "            # Get tokens routed to this expert from any top-k position\n",
        "            token_indices = torch.any(expert_mask_flat, dim=1).nonzero(as_tuple=True)[0]\n",
        "\n",
        "            if len(token_indices) > 0:\n",
        "                # Get the tokens for this expert\n",
        "                expert_tokens = x_flat[token_indices]  # [num_expert_tokens, input_dim]\n",
        "\n",
        "                # Process all tokens through this expert at once (BATCHED!)\n",
        "                expert_out = self.experts[expert_idx](expert_tokens)  # [num_expert_tokens, input_dim]\n",
        "\n",
        "                # Get the weights for these tokens\n",
        "                # Find which top-k position this expert is at for each token\n",
        "                for k in range(self.top_k):\n",
        "                    k_mask = expert_mask_flat[:, k]  # [num_tokens]\n",
        "                    k_indices = k_mask.nonzero(as_tuple=True)[0]\n",
        "\n",
        "                    if len(k_indices) > 0:\n",
        "                        # Find position in expert_tokens\n",
        "                        positions = torch.searchsorted(token_indices, k_indices)\n",
        "                        weights = top_k_probs[k_indices, k].unsqueeze(1)  # [num_k_tokens, 1]\n",
        "\n",
        "                        # Add weighted expert output\n",
        "                        output[k_indices] += weights * expert_out[positions]\n",
        "\n",
        "        # ====================================================================\n",
        "        # SHARED EXPERTS: OPTIMIZED BATCHED PROCESSING\n",
        "        # ====================================================================\n",
        "        shared_output = torch.zeros_like(x_flat)\n",
        "        for shared_expert in self.shared_experts:\n",
        "            # Process all tokens at once\n",
        "            shared_output += shared_expert(x_flat)\n",
        "        shared_output = shared_output / self.num_shared_experts\n",
        "\n",
        "        # Combine sparse and shared expert outputs\n",
        "        output = output + shared_output\n",
        "\n",
        "        # Reshape back\n",
        "        output = output.view(batch_size, seq_len, input_dim)\n",
        "\n",
        "        # ====================================================================\n",
        "        # AUXILIARY LOSSES\n",
        "        # ====================================================================\n",
        "        # Load balancing loss\n",
        "        load_balance_loss = self.compute_load_balance_loss(\n",
        "            router_probs.view(batch_size, seq_len, self.num_experts),\n",
        "            expert_mask\n",
        "        )\n",
        "\n",
        "        # Router Z-loss\n",
        "        z_loss = self.compute_router_z_loss(router_logits)\n",
        "\n",
        "        # Compute auxiliary loss and keep as 1D tensor for DataParallel\n",
        "        aux_loss = (\n",
        "            self.load_balance_weight * load_balance_loss +\n",
        "            self.z_loss_weight * z_loss\n",
        "        ).unsqueeze(0)  # Shape: [1] instead of scalar\n",
        "\n",
        "        return output, aux_loss\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# COMPLETE MOE MODEL - FIXED\n",
        "# ============================================================================\n",
        "\n",
        "class MoETransformerBlock(nn.Module):\n",
        "    \"\"\"Transformer block with MoE layer - FIXED\"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        embed_dim: int,\n",
        "        num_heads: int,\n",
        "        moe_config: dict,\n",
        "        dropout: float = 0.1\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        # Self-attention\n",
        "        self.attention = nn.MultiheadAttention(\n",
        "            embed_dim,\n",
        "            num_heads,\n",
        "            dropout=dropout,\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.norm1 = nn.LayerNorm(embed_dim)\n",
        "\n",
        "        # MoE layer\n",
        "        self.moe = MixtureOfExpertsLayer(\n",
        "            input_dim=embed_dim,\n",
        "            **moe_config\n",
        "        )\n",
        "        self.norm2 = nn.LayerNorm(embed_dim)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x: torch.Tensor, mask: Optional[torch.Tensor] = None):\n",
        "        # Self-attention with residual\n",
        "        attn_out, _ = self.attention(x, x, x, attn_mask=mask)\n",
        "        x = self.norm1(x + self.dropout(attn_out))\n",
        "\n",
        "        # MoE with residual - unpack tuple\n",
        "        moe_out, aux_loss = self.moe(x)\n",
        "        x = self.norm2(x + self.dropout(moe_out))\n",
        "\n",
        "        return x, aux_loss\n",
        "\n",
        "\n",
        "class MoEModel(nn.Module):\n",
        "    \"\"\"Complete MoE model for sequence tasks - FIXED\"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        vocab_size: int,\n",
        "        embed_dim: int = 256,\n",
        "        num_layers: int = 4,\n",
        "        num_heads: int = 4,\n",
        "        num_experts: int = 8,\n",
        "        num_shared_experts: int = 2,\n",
        "        top_k: int = 2,\n",
        "        hidden_dim: int = 512,\n",
        "        max_seq_len: int = 512,\n",
        "        dropout: float = 0.1\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.embed_dim = embed_dim\n",
        "\n",
        "        # Embeddings\n",
        "        self.token_embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.position_embedding = nn.Embedding(max_seq_len, embed_dim)\n",
        "\n",
        "        # MoE config\n",
        "        moe_config = {\n",
        "            'hidden_dim': hidden_dim,\n",
        "            'num_experts': num_experts,\n",
        "            'num_shared_experts': num_shared_experts,\n",
        "            'top_k': top_k,\n",
        "            'dropout': dropout\n",
        "        }\n",
        "\n",
        "        # Transformer blocks with MoE\n",
        "        self.layers = nn.ModuleList([\n",
        "            MoETransformerBlock(embed_dim, num_heads, moe_config, dropout)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "        # Output head\n",
        "        self.output_norm = nn.LayerNorm(embed_dim)\n",
        "        self.output_projection = nn.Linear(embed_dim, vocab_size)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # Initialize weights\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, input_ids: torch.Tensor, mask: Optional[torch.Tensor] = None):\n",
        "        batch_size, seq_len = input_ids.shape\n",
        "\n",
        "        # Embeddings\n",
        "        positions = torch.arange(seq_len, device=input_ids.device).unsqueeze(0)\n",
        "        x = self.token_embedding(input_ids) + self.position_embedding(positions)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        # Accumulate auxiliary losses\n",
        "        total_aux_loss = 0.0\n",
        "\n",
        "        # Pass through transformer blocks\n",
        "        for layer in self.layers:\n",
        "            x, aux_loss = layer(x, mask)\n",
        "            total_aux_loss += aux_loss\n",
        "\n",
        "        # Output\n",
        "        x = self.output_norm(x)\n",
        "        logits = self.output_projection(x)\n",
        "\n",
        "        return logits, total_aux_loss\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# SHAKESPEARE DATASET\n",
        "# ============================================================================\n",
        "\n",
        "class ShakespeareDataset(Dataset):\n",
        "    \"\"\"Shakespeare dataset with data_fraction support\"\"\"\n",
        "    def __init__(self, text_filepath: str, vocab: dict, seq_len: int, data_fraction: float = 1.0):\n",
        "        self.vocab = vocab\n",
        "        self.seq_len = seq_len\n",
        "        self.data_fraction = data_fraction\n",
        "\n",
        "        # Read and tokenize the entire text\n",
        "        with open(text_filepath, 'r', encoding='utf-8') as f:\n",
        "            raw_text = f.read()\n",
        "\n",
        "        # Tokenize (using global tokenize function)\n",
        "        full_tokenized_text = self.tokenize_text(raw_text, self.vocab)\n",
        "\n",
        "        # Calculate number of tokens to use based on data_fraction\n",
        "        num_tokens_to_use = int(len(full_tokenized_text) * self.data_fraction)\n",
        "        self.tokenized_text = full_tokenized_text[:num_tokens_to_use]\n",
        "\n",
        "        if len(self.tokenized_text) < self.seq_len + 1:\n",
        "            raise ValueError(\"Text is too short for the specified sequence length after applying data_fraction.\")\n",
        "\n",
        "        print(f\"ShakespeareDataset initialized:\")\n",
        "        print(f\"  Using data fraction: {self.data_fraction:.2f}\")\n",
        "        print(f\"  Total tokens (after fraction): {len(self.tokenized_text)}\")\n",
        "        print(f\"  Sequence length: {self.seq_len}\")\n",
        "        print(f\"  Number of available sequences: {len(self)}\")\n",
        "\n",
        "    def tokenize_text(self, text_string, vocab):\n",
        "        \"\"\"Tokenize text string to list of token IDs\"\"\"\n",
        "        text_string = text_string.lower()\n",
        "        text_string = re.sub(r'[^a-z\\s]', ' ', text_string)\n",
        "        text_string = re.sub(r'\\s+', ' ', text_string).strip()\n",
        "\n",
        "        tokens = text_string.split()\n",
        "        token_ids = [vocab.get(token, vocab.get(\"<unk>\", 1)) for token in tokens]\n",
        "\n",
        "        return token_ids\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.tokenized_text) - self.seq_len\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Input sequence: tokens from `idx` to `idx + seq_len`\n",
        "        input_sequence = self.tokenized_text[idx : idx + self.seq_len]\n",
        "\n",
        "        # Target sequence: tokens from `idx + 1` to `idx + seq_len + 1`\n",
        "        target_sequence = self.tokenized_text[idx + 1 : idx + self.seq_len + 1]\n",
        "\n",
        "        # Convert to torch.Tensor\n",
        "        input_tensor = torch.tensor(input_sequence, dtype=torch.long)\n",
        "        target_tensor = torch.tensor(target_sequence, dtype=torch.long)\n",
        "\n",
        "        return input_tensor, target_tensor\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# TRAINING UTILITIES - FIXED\n",
        "# ============================================================================\n",
        "\n",
        "def train_step(model, inputs, targets, optimizer, device):\n",
        "    \"\"\"Single training step - FIXED to handle DataParallel without warnings\"\"\"\n",
        "    model.train()\n",
        "\n",
        "    # Move to device\n",
        "    inputs = inputs.to(device)\n",
        "    targets = targets.to(device)\n",
        "\n",
        "    # Forward pass\n",
        "    logits, aux_loss = model(inputs)\n",
        "\n",
        "    # IMPORTANT FIX: Handle DataParallel output\n",
        "    # DataParallel returns a tensor of shape [num_gpus] with one value per GPU\n",
        "    # We need to average these values to get a single scalar\n",
        "    aux_loss = aux_loss.mean()\n",
        "\n",
        "    # Compute main loss (cross-entropy)\n",
        "    main_loss = F.cross_entropy(\n",
        "        logits.reshape(-1, logits.size(-1)),\n",
        "        targets.reshape(-1)\n",
        "    )\n",
        "\n",
        "    # Total loss includes auxiliary losses\n",
        "    total_loss = main_loss + aux_loss\n",
        "\n",
        "    # Backward pass\n",
        "    optimizer.zero_grad()\n",
        "    total_loss.backward()\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "    optimizer.step()\n",
        "\n",
        "    return {\n",
        "        'total_loss': total_loss.item(),\n",
        "        'main_loss': main_loss.item(),\n",
        "        'aux_loss': aux_loss.item()\n",
        "    }\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# MULTI-GPU SUPPORT\n",
        "# ============================================================================\n",
        "\n",
        "def setup_distributed_model(model, device_count):\n",
        "    \"\"\"Setup model for multi-GPU training\"\"\"\n",
        "    if device_count > 1:\n",
        "        print(f\"Using DataParallel with {device_count} GPUs\")\n",
        "        model = nn.DataParallel(model)\n",
        "    return model\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# DOWNLOAD SHAKESPEARE DATA\n",
        "# ============================================================================\n",
        "\n",
        "def download_shakespeare(url='https://www.gutenberg.org/files/100/100-0.txt', filename='shakespeare.txt'):\n",
        "    \"\"\"Download Shakespeare's complete works\"\"\"\n",
        "    print(f\"Downloading Shakespeare from {url}...\")\n",
        "    try:\n",
        "        response = requests.get(url, stream=True)\n",
        "        response.raise_for_status()\n",
        "\n",
        "        with open(filename, 'wb') as f:\n",
        "            for chunk in response.iter_content(chunk_size=8192):\n",
        "                f.write(chunk)\n",
        "\n",
        "        print(f\"Successfully downloaded to '{filename}'\")\n",
        "        return True\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error downloading: {e}\")\n",
        "        return False\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# BUILD VOCABULARY\n",
        "# ============================================================================\n",
        "\n",
        "def build_vocabulary(text_filepath, vocab_size_limit=10000):\n",
        "    \"\"\"Build vocabulary from text file\"\"\"\n",
        "    print(f\"Building vocabulary from {text_filepath}...\")\n",
        "\n",
        "    with open(text_filepath, 'r', encoding='utf-8') as f:\n",
        "        text = f.read()\n",
        "\n",
        "    # Preprocess\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^a-z\\s]', ' ', text)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    words = text.split()\n",
        "    print(f\"Total words: {len(words)}\")\n",
        "\n",
        "    # Create vocabulary\n",
        "    word_counts = Counter(words)\n",
        "    most_common_words = [word for word, _ in word_counts.most_common(vocab_size_limit - 2)]\n",
        "\n",
        "    vocabulary = {\"<pad>\": 0, \"<unk>\": 1}\n",
        "    for i, word in enumerate(most_common_words):\n",
        "        vocabulary[word] = i + 2\n",
        "\n",
        "    print(f\"Vocabulary size: {len(vocabulary)}\")\n",
        "\n",
        "    return vocabulary\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# MAIN TRAINING LOOP - FIXED\n",
        "# ============================================================================\n",
        "\n",
        "def main():\n",
        "    # Hyperparameters\n",
        "    TEXT_FILEPATH = 'shakespeare.txt'\n",
        "    SEQUENCE_LENGTH = 64\n",
        "    BATCH_SIZE = 32\n",
        "    vocab_size_limit = 10000\n",
        "\n",
        "    embed_dim = 256\n",
        "    num_layers = 4\n",
        "    num_heads = 4\n",
        "    num_experts = 8\n",
        "    num_shared_experts = 2\n",
        "    top_k = 2\n",
        "    hidden_dim = 512\n",
        "    num_epochs = 50\n",
        "    learning_rate = 3e-4\n",
        "    data_fraction = 0.01  # Use 10% of data for faster training\n",
        "\n",
        "    # Download Shakespeare data if not exists\n",
        "    import os\n",
        "    if not os.path.exists(TEXT_FILEPATH):\n",
        "        download_shakespeare(filename=TEXT_FILEPATH)\n",
        "\n",
        "    # Build vocabulary\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"INITIALIZATION PHASE\")\n",
        "    print(\"=\"*60)\n",
        "    vocabulary = build_vocabulary(TEXT_FILEPATH, vocab_size_limit)\n",
        "    vocab_size = len(vocabulary)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"MODEL CREATION PHASE\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Create model\n",
        "    model = MoEModel(\n",
        "        vocab_size=vocab_size,\n",
        "        embed_dim=embed_dim,\n",
        "        num_layers=num_layers,\n",
        "        num_heads=num_heads,\n",
        "        num_experts=num_experts,\n",
        "        num_shared_experts=num_shared_experts,\n",
        "        top_k=top_k,\n",
        "        hidden_dim=hidden_dim\n",
        "    )\n",
        "\n",
        "    # Setup multi-GPU if available\n",
        "    model = setup_distributed_model(model, device_count)\n",
        "    model = model.to(device)\n",
        "\n",
        "    # Print model info\n",
        "    num_params = sum(p.numel() for p in model.parameters())\n",
        "    print(f\"\\nModel Parameters: {num_params:,}\")\n",
        "    print(f\"Number of Experts: {num_experts}\")\n",
        "    print(f\"Number of Shared Experts: {num_shared_experts}\")\n",
        "    print(f\"Top-K Experts: {top_k}\")\n",
        "\n",
        "    # Create dataset and dataloader\n",
        "    dataset = ShakespeareDataset(TEXT_FILEPATH, vocabulary, SEQUENCE_LENGTH, data_fraction=data_fraction)\n",
        "    dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "    # Optimizer\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    # Training loop\n",
        "    print(\"\\nStarting training...\")\n",
        "    for epoch in range(num_epochs):\n",
        "        epoch_losses = {'total': 0, 'main': 0, 'aux': 0}\n",
        "\n",
        "        for i, (inputs, targets) in enumerate(dataloader):\n",
        "            losses = train_step(model, inputs, targets, optimizer, device)\n",
        "\n",
        "            epoch_losses['total'] += losses['total_loss']\n",
        "            epoch_losses['main'] += losses['main_loss']\n",
        "            epoch_losses['aux'] += losses['aux_loss']\n",
        "\n",
        "            if (i + 1) % 100 == 0:\n",
        "                print(f\"Epoch {epoch+1}/{num_epochs}, Step {i+1}/{len(dataloader)}, \"\n",
        "                      f\"Loss: {losses['total_loss']:.4f} \"\n",
        "                      f\"(Main: {losses['main_loss']:.4f}, Aux: {losses['aux_loss']:.4f})\")\n",
        "\n",
        "        avg_losses = {k: v / len(dataloader) for k, v in epoch_losses.items()}\n",
        "        print(f\"\\nEpoch {epoch+1} Summary - \"\n",
        "              f\"Avg Loss: {avg_losses['total']:.4f} \"\n",
        "              f\"(Main: {avg_losses['main']:.4f}, Aux: {avg_losses['aux']:.4f})\\n\")\n",
        "\n",
        "    print(\"Training complete!\")\n",
        "\n",
        "    return model\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GglXfRck_t1-",
        "outputId": "14b02e14-7ffa-44af-db77-a8e542ff7eb2"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Available GPUs: 1\n",
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ============================================================================\n",
        "# RUN TRAINING\n",
        "# ============================================================================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    model = main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E5DAP2rm0so6",
        "outputId": "d701d549-341d-4b77-d2a7-c55c757448e6"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "INITIALIZATION PHASE\n",
            "============================================================\n",
            "Building vocabulary from shakespeare.txt...\n",
            "Total words: 988682\n",
            "Vocabulary size: 10000\n",
            "\n",
            "============================================================\n",
            "MODEL CREATION PHASE\n",
            "============================================================\n",
            "\n",
            "Model Parameters: 16,843,024\n",
            "Number of Experts: 8\n",
            "Number of Shared Experts: 2\n",
            "Top-K Experts: 2\n",
            "ShakespeareDataset initialized:\n",
            "  Using data fraction: 0.01\n",
            "  Total tokens (after fraction): 9886\n",
            "  Sequence length: 64\n",
            "  Number of available sequences: 9822\n",
            "\n",
            "Starting training...\n",
            "Epoch 1/50, Step 100/307, Loss: 5.4118 (Main: 5.3254, Aux: 0.0864)\n",
            "Epoch 1/50, Step 200/307, Loss: 3.7014 (Main: 3.6155, Aux: 0.0859)\n",
            "Epoch 1/50, Step 300/307, Loss: 2.7130 (Main: 2.6275, Aux: 0.0856)\n",
            "\n",
            "Epoch 1 Summary - Avg Loss: 4.6691 (Main: 4.5829, Aux: 0.0861)\n",
            "\n",
            "Epoch 2/50, Step 100/307, Loss: 1.9306 (Main: 1.8441, Aux: 0.0865)\n",
            "Epoch 2/50, Step 200/307, Loss: 1.2212 (Main: 1.1323, Aux: 0.0889)\n",
            "Epoch 2/50, Step 300/307, Loss: 0.8007 (Main: 0.7128, Aux: 0.0879)\n",
            "\n",
            "Epoch 2 Summary - Avg Loss: 1.5582 (Main: 1.4707, Aux: 0.0875)\n",
            "\n",
            "Epoch 3/50, Step 100/307, Loss: 0.5720 (Main: 0.4849, Aux: 0.0871)\n",
            "Epoch 3/50, Step 200/307, Loss: 0.4189 (Main: 0.3337, Aux: 0.0852)\n",
            "Epoch 3/50, Step 300/307, Loss: 0.3231 (Main: 0.2384, Aux: 0.0847)\n",
            "\n",
            "Epoch 3 Summary - Avg Loss: 0.4980 (Main: 0.4116, Aux: 0.0864)\n",
            "\n",
            "Epoch 4/50, Step 100/307, Loss: 0.2791 (Main: 0.1948, Aux: 0.0842)\n",
            "Epoch 4/50, Step 200/307, Loss: 0.2387 (Main: 0.1552, Aux: 0.0834)\n",
            "Epoch 4/50, Step 300/307, Loss: 0.2163 (Main: 0.1333, Aux: 0.0830)\n",
            "\n",
            "Epoch 4 Summary - Avg Loss: 0.2594 (Main: 0.1752, Aux: 0.0842)\n",
            "\n",
            "Epoch 5/50, Step 100/307, Loss: 0.1966 (Main: 0.1134, Aux: 0.0832)\n",
            "Epoch 5/50, Step 200/307, Loss: 0.1786 (Main: 0.0941, Aux: 0.0846)\n",
            "Epoch 5/50, Step 300/307, Loss: 0.1611 (Main: 0.0779, Aux: 0.0832)\n",
            "\n",
            "Epoch 5 Summary - Avg Loss: 0.1831 (Main: 0.0998, Aux: 0.0833)\n",
            "\n",
            "Epoch 6/50, Step 100/307, Loss: 0.1517 (Main: 0.0688, Aux: 0.0829)\n",
            "Epoch 6/50, Step 200/307, Loss: 0.1567 (Main: 0.0742, Aux: 0.0824)\n",
            "Epoch 6/50, Step 300/307, Loss: 0.1372 (Main: 0.0548, Aux: 0.0824)\n",
            "\n",
            "Epoch 6 Summary - Avg Loss: 0.1517 (Main: 0.0690, Aux: 0.0827)\n",
            "\n",
            "Epoch 7/50, Step 100/307, Loss: 0.1372 (Main: 0.0549, Aux: 0.0823)\n",
            "Epoch 7/50, Step 200/307, Loss: 0.1238 (Main: 0.0409, Aux: 0.0829)\n",
            "Epoch 7/50, Step 300/307, Loss: 0.1428 (Main: 0.0606, Aux: 0.0821)\n",
            "\n",
            "Epoch 7 Summary - Avg Loss: 0.1355 (Main: 0.0530, Aux: 0.0824)\n",
            "\n",
            "Epoch 8/50, Step 100/307, Loss: 0.1220 (Main: 0.0394, Aux: 0.0826)\n",
            "Epoch 8/50, Step 200/307, Loss: 0.1319 (Main: 0.0501, Aux: 0.0818)\n",
            "Epoch 8/50, Step 300/307, Loss: 0.1308 (Main: 0.0491, Aux: 0.0817)\n",
            "\n",
            "Epoch 8 Summary - Avg Loss: 0.1246 (Main: 0.0424, Aux: 0.0822)\n",
            "\n",
            "Epoch 9/50, Step 100/307, Loss: 0.1181 (Main: 0.0362, Aux: 0.0819)\n",
            "Epoch 9/50, Step 200/307, Loss: 0.1087 (Main: 0.0273, Aux: 0.0814)\n",
            "Epoch 9/50, Step 300/307, Loss: 0.1079 (Main: 0.0245, Aux: 0.0834)\n",
            "\n",
            "Epoch 9 Summary - Avg Loss: 0.1176 (Main: 0.0356, Aux: 0.0821)\n",
            "\n",
            "Epoch 10/50, Step 100/307, Loss: 0.1093 (Main: 0.0280, Aux: 0.0813)\n",
            "Epoch 10/50, Step 200/307, Loss: 0.1155 (Main: 0.0333, Aux: 0.0822)\n",
            "Epoch 10/50, Step 300/307, Loss: 0.1140 (Main: 0.0324, Aux: 0.0816)\n",
            "\n",
            "Epoch 10 Summary - Avg Loss: 0.1109 (Main: 0.0290, Aux: 0.0820)\n",
            "\n",
            "Epoch 11/50, Step 100/307, Loss: 0.1036 (Main: 0.0207, Aux: 0.0829)\n",
            "Epoch 11/50, Step 200/307, Loss: 0.1033 (Main: 0.0221, Aux: 0.0812)\n",
            "Epoch 11/50, Step 300/307, Loss: 0.1188 (Main: 0.0371, Aux: 0.0817)\n",
            "\n",
            "Epoch 11 Summary - Avg Loss: 0.1066 (Main: 0.0247, Aux: 0.0819)\n",
            "\n",
            "Epoch 12/50, Step 100/307, Loss: 0.0989 (Main: 0.0169, Aux: 0.0819)\n",
            "Epoch 12/50, Step 200/307, Loss: 0.1129 (Main: 0.0308, Aux: 0.0821)\n",
            "Epoch 12/50, Step 300/307, Loss: 0.1029 (Main: 0.0213, Aux: 0.0816)\n",
            "\n",
            "Epoch 12 Summary - Avg Loss: 0.1034 (Main: 0.0215, Aux: 0.0819)\n",
            "\n",
            "Epoch 13/50, Step 100/307, Loss: 0.0992 (Main: 0.0174, Aux: 0.0819)\n",
            "Epoch 13/50, Step 200/307, Loss: 0.0974 (Main: 0.0162, Aux: 0.0812)\n",
            "Epoch 13/50, Step 300/307, Loss: 0.0909 (Main: 0.0091, Aux: 0.0818)\n",
            "\n",
            "Epoch 13 Summary - Avg Loss: 0.0990 (Main: 0.0172, Aux: 0.0818)\n",
            "\n",
            "Epoch 14/50, Step 100/307, Loss: 0.1034 (Main: 0.0220, Aux: 0.0814)\n",
            "Epoch 14/50, Step 200/307, Loss: 0.0963 (Main: 0.0144, Aux: 0.0819)\n",
            "Epoch 14/50, Step 300/307, Loss: 0.1022 (Main: 0.0205, Aux: 0.0817)\n",
            "\n",
            "Epoch 14 Summary - Avg Loss: 0.0976 (Main: 0.0159, Aux: 0.0818)\n",
            "\n",
            "Epoch 15/50, Step 100/307, Loss: 0.1001 (Main: 0.0179, Aux: 0.0822)\n",
            "Epoch 15/50, Step 200/307, Loss: 0.0949 (Main: 0.0132, Aux: 0.0817)\n",
            "Epoch 15/50, Step 300/307, Loss: 0.0933 (Main: 0.0112, Aux: 0.0821)\n",
            "\n",
            "Epoch 15 Summary - Avg Loss: 0.0953 (Main: 0.0136, Aux: 0.0817)\n",
            "\n",
            "Epoch 16/50, Step 100/307, Loss: 0.0968 (Main: 0.0148, Aux: 0.0821)\n",
            "Epoch 16/50, Step 200/307, Loss: 0.0942 (Main: 0.0128, Aux: 0.0814)\n",
            "Epoch 16/50, Step 300/307, Loss: 0.0911 (Main: 0.0099, Aux: 0.0812)\n",
            "\n",
            "Epoch 16 Summary - Avg Loss: 0.0939 (Main: 0.0122, Aux: 0.0816)\n",
            "\n",
            "Epoch 17/50, Step 100/307, Loss: 0.0899 (Main: 0.0087, Aux: 0.0812)\n",
            "Epoch 17/50, Step 200/307, Loss: 0.0986 (Main: 0.0162, Aux: 0.0824)\n",
            "Epoch 17/50, Step 300/307, Loss: 0.1020 (Main: 0.0198, Aux: 0.0822)\n",
            "\n",
            "Epoch 17 Summary - Avg Loss: 0.0938 (Main: 0.0121, Aux: 0.0816)\n",
            "\n",
            "Epoch 18/50, Step 100/307, Loss: 0.0950 (Main: 0.0135, Aux: 0.0815)\n",
            "Epoch 18/50, Step 200/307, Loss: 0.0958 (Main: 0.0149, Aux: 0.0810)\n",
            "Epoch 18/50, Step 300/307, Loss: 0.0884 (Main: 0.0072, Aux: 0.0812)\n",
            "\n",
            "Epoch 18 Summary - Avg Loss: 0.0920 (Main: 0.0105, Aux: 0.0815)\n",
            "\n",
            "Epoch 19/50, Step 100/307, Loss: 0.0909 (Main: 0.0099, Aux: 0.0810)\n",
            "Epoch 19/50, Step 200/307, Loss: 0.0906 (Main: 0.0090, Aux: 0.0817)\n",
            "Epoch 19/50, Step 300/307, Loss: 0.0870 (Main: 0.0063, Aux: 0.0808)\n",
            "\n",
            "Epoch 19 Summary - Avg Loss: 0.0906 (Main: 0.0091, Aux: 0.0815)\n",
            "\n",
            "Epoch 20/50, Step 100/307, Loss: 0.0947 (Main: 0.0126, Aux: 0.0821)\n",
            "Epoch 20/50, Step 200/307, Loss: 0.0894 (Main: 0.0082, Aux: 0.0811)\n",
            "Epoch 20/50, Step 300/307, Loss: 0.0910 (Main: 0.0098, Aux: 0.0812)\n",
            "\n",
            "Epoch 20 Summary - Avg Loss: 0.0903 (Main: 0.0089, Aux: 0.0815)\n",
            "\n",
            "Epoch 21/50, Step 100/307, Loss: 0.0895 (Main: 0.0083, Aux: 0.0812)\n",
            "Epoch 21/50, Step 200/307, Loss: 0.0892 (Main: 0.0073, Aux: 0.0819)\n",
            "Epoch 21/50, Step 300/307, Loss: 0.0910 (Main: 0.0094, Aux: 0.0816)\n",
            "\n",
            "Epoch 21 Summary - Avg Loss: 0.0891 (Main: 0.0077, Aux: 0.0814)\n",
            "\n",
            "Epoch 22/50, Step 100/307, Loss: 0.0916 (Main: 0.0097, Aux: 0.0818)\n",
            "Epoch 22/50, Step 200/307, Loss: 0.0873 (Main: 0.0058, Aux: 0.0815)\n",
            "Epoch 22/50, Step 300/307, Loss: 0.0869 (Main: 0.0057, Aux: 0.0812)\n",
            "\n",
            "Epoch 22 Summary - Avg Loss: 0.0886 (Main: 0.0072, Aux: 0.0814)\n",
            "\n",
            "Epoch 23/50, Step 100/307, Loss: 0.0875 (Main: 0.0049, Aux: 0.0826)\n",
            "Epoch 23/50, Step 200/307, Loss: 0.0843 (Main: 0.0033, Aux: 0.0810)\n",
            "Epoch 23/50, Step 300/307, Loss: 0.0855 (Main: 0.0039, Aux: 0.0816)\n",
            "\n",
            "Epoch 23 Summary - Avg Loss: 0.0873 (Main: 0.0060, Aux: 0.0812)\n",
            "\n",
            "Epoch 24/50, Step 100/307, Loss: 0.0835 (Main: 0.0028, Aux: 0.0807)\n",
            "Epoch 24/50, Step 200/307, Loss: 0.0882 (Main: 0.0072, Aux: 0.0809)\n",
            "Epoch 24/50, Step 300/307, Loss: 0.0870 (Main: 0.0050, Aux: 0.0821)\n",
            "\n",
            "Epoch 24 Summary - Avg Loss: 0.0868 (Main: 0.0056, Aux: 0.0812)\n",
            "\n",
            "Epoch 25/50, Step 100/307, Loss: 0.0842 (Main: 0.0032, Aux: 0.0811)\n",
            "Epoch 25/50, Step 200/307, Loss: 0.0845 (Main: 0.0033, Aux: 0.0812)\n",
            "Epoch 25/50, Step 300/307, Loss: 0.0827 (Main: 0.0019, Aux: 0.0808)\n",
            "\n",
            "Epoch 25 Summary - Avg Loss: 0.0862 (Main: 0.0050, Aux: 0.0812)\n",
            "\n",
            "Epoch 26/50, Step 100/307, Loss: 0.0893 (Main: 0.0080, Aux: 0.0812)\n",
            "Epoch 26/50, Step 200/307, Loss: 0.0822 (Main: 0.0014, Aux: 0.0809)\n",
            "Epoch 26/50, Step 300/307, Loss: 0.0863 (Main: 0.0056, Aux: 0.0806)\n",
            "\n",
            "Epoch 26 Summary - Avg Loss: 0.0860 (Main: 0.0049, Aux: 0.0811)\n",
            "\n",
            "Epoch 27/50, Step 100/307, Loss: 0.0844 (Main: 0.0033, Aux: 0.0812)\n",
            "Epoch 27/50, Step 200/307, Loss: 0.0884 (Main: 0.0071, Aux: 0.0813)\n",
            "Epoch 27/50, Step 300/307, Loss: 0.0877 (Main: 0.0066, Aux: 0.0811)\n",
            "\n",
            "Epoch 27 Summary - Avg Loss: 0.0850 (Main: 0.0040, Aux: 0.0810)\n",
            "\n",
            "Epoch 28/50, Step 100/307, Loss: 0.0870 (Main: 0.0061, Aux: 0.0809)\n",
            "Epoch 28/50, Step 200/307, Loss: 0.0874 (Main: 0.0065, Aux: 0.0809)\n",
            "Epoch 28/50, Step 300/307, Loss: 0.0880 (Main: 0.0065, Aux: 0.0815)\n",
            "\n",
            "Epoch 28 Summary - Avg Loss: 0.0853 (Main: 0.0044, Aux: 0.0810)\n",
            "\n",
            "Epoch 29/50, Step 100/307, Loss: 0.0857 (Main: 0.0045, Aux: 0.0812)\n",
            "Epoch 29/50, Step 200/307, Loss: 0.0889 (Main: 0.0078, Aux: 0.0812)\n",
            "Epoch 29/50, Step 300/307, Loss: 0.0835 (Main: 0.0023, Aux: 0.0813)\n",
            "\n",
            "Epoch 29 Summary - Avg Loss: 0.0854 (Main: 0.0044, Aux: 0.0810)\n",
            "\n",
            "Epoch 30/50, Step 100/307, Loss: 0.0835 (Main: 0.0026, Aux: 0.0809)\n",
            "Epoch 30/50, Step 200/307, Loss: 0.0907 (Main: 0.0099, Aux: 0.0809)\n",
            "Epoch 30/50, Step 300/307, Loss: 0.0849 (Main: 0.0038, Aux: 0.0811)\n",
            "\n",
            "Epoch 30 Summary - Avg Loss: 0.0847 (Main: 0.0038, Aux: 0.0809)\n",
            "\n",
            "Epoch 31/50, Step 100/307, Loss: 0.0841 (Main: 0.0032, Aux: 0.0809)\n",
            "Epoch 31/50, Step 200/307, Loss: 0.0825 (Main: 0.0015, Aux: 0.0810)\n",
            "Epoch 31/50, Step 300/307, Loss: 0.0823 (Main: 0.0014, Aux: 0.0809)\n",
            "\n",
            "Epoch 31 Summary - Avg Loss: 0.0840 (Main: 0.0032, Aux: 0.0808)\n",
            "\n",
            "Epoch 32/50, Step 100/307, Loss: 0.0833 (Main: 0.0024, Aux: 0.0810)\n",
            "Epoch 32/50, Step 200/307, Loss: 0.0837 (Main: 0.0030, Aux: 0.0807)\n",
            "Epoch 32/50, Step 300/307, Loss: 0.0872 (Main: 0.0066, Aux: 0.0806)\n",
            "\n",
            "Epoch 32 Summary - Avg Loss: 0.0844 (Main: 0.0036, Aux: 0.0808)\n",
            "\n",
            "Epoch 33/50, Step 100/307, Loss: 0.0820 (Main: 0.0013, Aux: 0.0807)\n",
            "Epoch 33/50, Step 200/307, Loss: 0.0898 (Main: 0.0091, Aux: 0.0806)\n",
            "Epoch 33/50, Step 300/307, Loss: 0.0857 (Main: 0.0047, Aux: 0.0809)\n",
            "\n",
            "Epoch 33 Summary - Avg Loss: 0.0842 (Main: 0.0034, Aux: 0.0808)\n",
            "\n",
            "Epoch 34/50, Step 100/307, Loss: 0.0812 (Main: 0.0005, Aux: 0.0807)\n",
            "Epoch 34/50, Step 200/307, Loss: 0.0835 (Main: 0.0028, Aux: 0.0807)\n",
            "Epoch 34/50, Step 300/307, Loss: 0.0830 (Main: 0.0024, Aux: 0.0806)\n",
            "\n",
            "Epoch 34 Summary - Avg Loss: 0.0840 (Main: 0.0032, Aux: 0.0807)\n",
            "\n",
            "Epoch 35/50, Step 100/307, Loss: 0.0836 (Main: 0.0031, Aux: 0.0805)\n",
            "Epoch 35/50, Step 200/307, Loss: 0.0826 (Main: 0.0021, Aux: 0.0805)\n",
            "Epoch 35/50, Step 300/307, Loss: 0.0829 (Main: 0.0019, Aux: 0.0810)\n",
            "\n",
            "Epoch 35 Summary - Avg Loss: 0.0841 (Main: 0.0034, Aux: 0.0807)\n",
            "\n",
            "Epoch 36/50, Step 100/307, Loss: 0.0824 (Main: 0.0015, Aux: 0.0810)\n",
            "Epoch 36/50, Step 200/307, Loss: 0.0845 (Main: 0.0038, Aux: 0.0808)\n",
            "Epoch 36/50, Step 300/307, Loss: 0.0822 (Main: 0.0014, Aux: 0.0808)\n",
            "\n",
            "Epoch 36 Summary - Avg Loss: 0.0837 (Main: 0.0030, Aux: 0.0807)\n",
            "\n",
            "Epoch 37/50, Step 100/307, Loss: 0.0835 (Main: 0.0030, Aux: 0.0805)\n",
            "Epoch 37/50, Step 200/307, Loss: 0.0821 (Main: 0.0010, Aux: 0.0810)\n",
            "Epoch 37/50, Step 300/307, Loss: 0.0856 (Main: 0.0049, Aux: 0.0807)\n",
            "\n",
            "Epoch 37 Summary - Avg Loss: 0.0837 (Main: 0.0030, Aux: 0.0806)\n",
            "\n",
            "Epoch 38/50, Step 100/307, Loss: 0.0834 (Main: 0.0026, Aux: 0.0808)\n",
            "Epoch 38/50, Step 200/307, Loss: 0.0832 (Main: 0.0028, Aux: 0.0805)\n",
            "Epoch 38/50, Step 300/307, Loss: 0.0817 (Main: 0.0012, Aux: 0.0805)\n",
            "\n",
            "Epoch 38 Summary - Avg Loss: 0.0838 (Main: 0.0031, Aux: 0.0806)\n",
            "\n",
            "Epoch 39/50, Step 100/307, Loss: 0.0856 (Main: 0.0046, Aux: 0.0810)\n",
            "Epoch 39/50, Step 200/307, Loss: 0.0835 (Main: 0.0024, Aux: 0.0811)\n",
            "Epoch 39/50, Step 300/307, Loss: 0.0845 (Main: 0.0036, Aux: 0.0808)\n",
            "\n",
            "Epoch 39 Summary - Avg Loss: 0.0833 (Main: 0.0027, Aux: 0.0806)\n",
            "\n",
            "Epoch 40/50, Step 100/307, Loss: 0.0812 (Main: 0.0005, Aux: 0.0807)\n",
            "Epoch 40/50, Step 200/307, Loss: 0.0865 (Main: 0.0059, Aux: 0.0806)\n",
            "Epoch 40/50, Step 300/307, Loss: 0.0823 (Main: 0.0018, Aux: 0.0806)\n",
            "\n",
            "Epoch 40 Summary - Avg Loss: 0.0841 (Main: 0.0035, Aux: 0.0806)\n",
            "\n",
            "Epoch 41/50, Step 100/307, Loss: 0.0820 (Main: 0.0015, Aux: 0.0805)\n",
            "Epoch 41/50, Step 200/307, Loss: 0.0829 (Main: 0.0025, Aux: 0.0804)\n",
            "Epoch 41/50, Step 300/307, Loss: 0.0819 (Main: 0.0013, Aux: 0.0806)\n",
            "\n",
            "Epoch 41 Summary - Avg Loss: 0.0832 (Main: 0.0027, Aux: 0.0805)\n",
            "\n",
            "Epoch 42/50, Step 100/307, Loss: 0.0814 (Main: 0.0009, Aux: 0.0805)\n",
            "Epoch 42/50, Step 200/307, Loss: 0.0823 (Main: 0.0019, Aux: 0.0805)\n",
            "Epoch 42/50, Step 300/307, Loss: 0.0862 (Main: 0.0057, Aux: 0.0805)\n",
            "\n",
            "Epoch 42 Summary - Avg Loss: 0.0833 (Main: 0.0027, Aux: 0.0805)\n",
            "\n",
            "Epoch 43/50, Step 100/307, Loss: 0.0815 (Main: 0.0011, Aux: 0.0804)\n",
            "Epoch 43/50, Step 200/307, Loss: 0.0829 (Main: 0.0024, Aux: 0.0805)\n",
            "Epoch 43/50, Step 300/307, Loss: 0.0844 (Main: 0.0041, Aux: 0.0803)\n",
            "\n",
            "Epoch 43 Summary - Avg Loss: 0.0831 (Main: 0.0025, Aux: 0.0805)\n",
            "\n",
            "Epoch 44/50, Step 100/307, Loss: 0.0812 (Main: 0.0005, Aux: 0.0807)\n",
            "Epoch 44/50, Step 200/307, Loss: 0.0823 (Main: 0.0019, Aux: 0.0804)\n",
            "Epoch 44/50, Step 300/307, Loss: 0.0838 (Main: 0.0032, Aux: 0.0806)\n",
            "\n",
            "Epoch 44 Summary - Avg Loss: 0.0827 (Main: 0.0022, Aux: 0.0805)\n",
            "\n",
            "Epoch 45/50, Step 100/307, Loss: 0.0824 (Main: 0.0017, Aux: 0.0807)\n",
            "Epoch 45/50, Step 200/307, Loss: 0.0818 (Main: 0.0015, Aux: 0.0803)\n",
            "Epoch 45/50, Step 300/307, Loss: 0.0827 (Main: 0.0021, Aux: 0.0806)\n",
            "\n",
            "Epoch 45 Summary - Avg Loss: 0.0826 (Main: 0.0021, Aux: 0.0805)\n",
            "\n",
            "Epoch 46/50, Step 100/307, Loss: 0.0831 (Main: 0.0025, Aux: 0.0806)\n",
            "Epoch 46/50, Step 200/307, Loss: 0.0839 (Main: 0.0033, Aux: 0.0806)\n",
            "Epoch 46/50, Step 300/307, Loss: 0.0835 (Main: 0.0029, Aux: 0.0806)\n",
            "\n",
            "Epoch 46 Summary - Avg Loss: 0.0828 (Main: 0.0023, Aux: 0.0805)\n",
            "\n",
            "Epoch 47/50, Step 100/307, Loss: 0.0824 (Main: 0.0018, Aux: 0.0806)\n",
            "Epoch 47/50, Step 200/307, Loss: 0.0816 (Main: 0.0013, Aux: 0.0802)\n",
            "Epoch 47/50, Step 300/307, Loss: 0.0829 (Main: 0.0024, Aux: 0.0804)\n",
            "\n",
            "Epoch 47 Summary - Avg Loss: 0.0830 (Main: 0.0025, Aux: 0.0805)\n",
            "\n",
            "Epoch 48/50, Step 100/307, Loss: 0.0854 (Main: 0.0051, Aux: 0.0803)\n",
            "Epoch 48/50, Step 200/307, Loss: 0.0846 (Main: 0.0042, Aux: 0.0803)\n",
            "Epoch 48/50, Step 300/307, Loss: 0.0812 (Main: 0.0006, Aux: 0.0805)\n",
            "\n",
            "Epoch 48 Summary - Avg Loss: 0.0831 (Main: 0.0026, Aux: 0.0805)\n",
            "\n",
            "Epoch 49/50, Step 100/307, Loss: 0.0824 (Main: 0.0018, Aux: 0.0806)\n",
            "Epoch 49/50, Step 200/307, Loss: 0.0819 (Main: 0.0016, Aux: 0.0804)\n",
            "Epoch 49/50, Step 300/307, Loss: 0.0821 (Main: 0.0014, Aux: 0.0807)\n",
            "\n",
            "Epoch 49 Summary - Avg Loss: 0.0829 (Main: 0.0024, Aux: 0.0805)\n",
            "\n",
            "Epoch 50/50, Step 100/307, Loss: 0.0815 (Main: 0.0012, Aux: 0.0803)\n",
            "Epoch 50/50, Step 200/307, Loss: 0.0850 (Main: 0.0047, Aux: 0.0803)\n",
            "Epoch 50/50, Step 300/307, Loss: 0.0818 (Main: 0.0013, Aux: 0.0805)\n",
            "\n",
            "Epoch 50 Summary - Avg Loss: 0.0830 (Main: 0.0025, Aux: 0.0805)\n",
            "\n",
            "Training complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5e43c3de"
      },
      "source": [
        "## Model Inference Example\n",
        "\n",
        "Now that the model has been trained, let's use it to generate some text based on a starting prompt. The `generate_text` function will take an initial sequence of tokens and predict the next tokens one by one, using temperature sampling to introduce some creativity."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e1751ec7",
        "outputId": "51aa9f64-4640-4f57-9351-0ca480d4b9c8"
      },
      "source": [
        "def generate_text(model, start_prompt: str, vocab: dict, id_to_word: dict, num_generate_tokens: int = 100, temperature: float = 0.8, seq_len: int = SEQUENCE_LENGTH):\n",
        "    model.eval() # Set model to evaluation mode\n",
        "\n",
        "    # Tokenize the starting prompt\n",
        "    input_ids = tokenize(start_prompt, vocab)\n",
        "\n",
        "    # If the prompt is longer than seq_len, truncate it\n",
        "    if len(input_ids) > seq_len:\n",
        "        input_ids = input_ids[-seq_len:]\n",
        "\n",
        "    # Pad the input if it's shorter than seq_len\n",
        "    if len(input_ids) < seq_len:\n",
        "        input_ids = input_ids + [vocab[\"<pad>\"]] * (seq_len - len(input_ids))\n",
        "\n",
        "    # Convert to tensor and add batch dimension\n",
        "    input_tensor = torch.tensor(input_ids, dtype=torch.long).unsqueeze(0).to(device)\n",
        "\n",
        "    generated_tokens = input_ids[:len(input_ids) - input_ids.count(vocab[\"<pad>\"])] # Store actual prompt tokens, removing initial padding\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for _ in range(num_generate_tokens):\n",
        "            # Get model predictions for the next token\n",
        "            # The model is trained to predict the next token given a sequence of length seq_len\n",
        "            logits, _ = model(input_tensor[:, -seq_len:])\n",
        "\n",
        "            # We only care about the prediction for the last token in the sequence\n",
        "            next_token_logits = logits[0, -1, :] / temperature\n",
        "\n",
        "            # Sample the next token\n",
        "            next_token = torch.multinomial(F.softmax(next_token_logits, dim=-1), num_samples=1).item()\n",
        "\n",
        "            # Add the new token to the sequence\n",
        "            input_tensor = torch.cat([\n",
        "                input_tensor[:, 1:],\n",
        "                torch.tensor([[next_token]], dtype=torch.long).to(device)\n",
        "            ], dim=1)\n",
        "\n",
        "            generated_tokens.append(next_token)\n",
        "\n",
        "            # Stop if <unk> or <pad> is generated (optional, adjust based on desired behavior)\n",
        "            if next_token == vocab[\"<unk>\"] or next_token == vocab[\"<pad>\"]:\n",
        "                break\n",
        "\n",
        "    # Convert generated token IDs back to words\n",
        "    generated_text = ' '.join([id_to_word.get(idx, '<unk>') for idx in generated_tokens])\n",
        "\n",
        "    return generated_text\n",
        "\n",
        "\n",
        "print(\"Defining generation function...\")\n",
        "\n",
        "# Example Usage:\n",
        "start_prompt = \"To be or not to be\"\n",
        "generated_output = generate_text(model, start_prompt, vocabulary, id_to_word, num_generate_tokens=150, temperature=0.7)\n",
        "\n",
        "print(f\"\\n--- Start Prompt: {start_prompt} ---\")\n",
        "print(f\"--- Generated Text: ---\\n{generated_output}\")\n",
        "\n",
        "start_prompt = \"Romeo, Romeo! wherefore art thou Romeo?\"\n",
        "generated_output = generate_text(model, start_prompt, vocabulary, id_to_word, num_generate_tokens=150, temperature=0.7)\n",
        "\n",
        "print(f\"\\n--- Start Prompt: {start_prompt} ---\")\n",
        "print(f\"--- Generated Text: ---\\n{generated_output}\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Defining generation function...\n",
            "\n",
            "--- Start Prompt: To be or not to be ---\n",
            "--- Generated Text: ---\n",
            "to be or not to be i <unk>\n",
            "\n",
            "--- Start Prompt: Romeo, Romeo! wherefore art thou Romeo? ---\n",
            "--- Generated Text: ---\n",
            "romeo romeo wherefore art thou romeo he i’d with hill and way as i school in but pedro that you saw son plum am place full when under a unwilling nether art or too loved lovell of sight monstrous thou the kneels is the cornwall thou the diamond doom art live my verona picture dishonest under to be thee but suck cease art downstairs presently but <unk>\n"
          ]
        }
      ]
    }
  ]
}